| distributed init (rank 0): env://, gpu 0
| distributed init (rank 6): env://, gpu 6| distributed init (rank 5): env://, gpu 5

| distributed init (rank 4): env://, gpu 4
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 7): env://, gpu 7
| distributed init (rank 3): env://, gpu 3
Namespace(batch_size=32, epochs=15, update_freq=4, model='base', drop_path=0.7, input_size=384, layer_scale_init_value=1e-06, model_ema=True, model_ema_decay=0.9999, model_ema_force_cpu=False, model_ema_eval=True, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=5.0, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=5e-05, layer_decay=1.0, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='./checkpoint_base_8.5G.pth', head_init_scale=1.0, model_key='model|module', model_prefix='', data_path='/dev/shm/imagenet', eval_data_path=None, nb_classes=1000, imagenet_default_mean_and_std=True, data_set='IMNET', output_dir='././checkpoint_base_256_11.1G', log_dir=None, device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, save_ckpt_freq=1, save_ckpt_num=3, start_epoch=0, eval=False, dist_eval=True, disable_eval=False, num_workers=10, pin_mem=True, world_size=8, local_rank=-1, dist_on_itp=False, dist_url='env://', use_amp=True, enable_wandb=False, project='convnext', wandb_ckpt=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Transform = 
RandomResizedCropAndInterpolation(size=(384, 384), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
RandomHorizontalFlip(p=0.5)
RandAugment(n=2, ops=
	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
ToTensor()
Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))
RandomErasing(p=0.25, mode=pixel, count=(1, 1))
---------------------------
reading from datapath /dev/shm/imagenet
Number of the class = 1000
Warping 384 size input images...
Transform = 
Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /dev/shm/imagenet
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f28e4b8dd50>
Load ckpt from ./checkpoint_base_8.5G.pth
Using EMA with decay = 0.99990000
Model = SPCNN(
  (first_conv): ConvX(
    (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): Act(
      (act): GELU(approximate='none')
    )
  )
  (layer1): Sequential(
    (0): DownBlock(
      (mlp): Sequential(
        (0): ConvX(
          (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (skip): Sequential(
        (0): ConvX(
          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
          (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
        (1): ConvX(
          (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (drop_path): Identity()
    )
    (1): BottleNeck(
      (drop_path): DropPath(drop_prob=0.020)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (2): BottleNeck(
      (drop_path): DropPath(drop_prob=0.040)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (3): BottleNeck(
      (drop_path): DropPath(drop_prob=0.060)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (4): BottleNeck(
      (drop_path): DropPath(drop_prob=0.080)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
              (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): DownBlock(
      (mlp): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (skip): Sequential(
        (0): ConvX(
          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
        (1): ConvX(
          (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (drop_path): DropPath(drop_prob=0.100)
    )
    (1): BottleNeck(
      (drop_path): DropPath(drop_prob=0.120)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (2): BottleNeck(
      (drop_path): DropPath(drop_prob=0.140)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (3): BottleNeck(
      (drop_path): DropPath(drop_prob=0.160)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (4): BottleNeck(
      (drop_path): DropPath(drop_prob=0.180)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (5): BottleNeck(
      (drop_path): DropPath(drop_prob=0.200)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (6): BottleNeck(
      (drop_path): DropPath(drop_prob=0.220)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (7): BottleNeck(
      (drop_path): DropPath(drop_prob=0.240)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (8): BottleNeck(
      (drop_path): DropPath(drop_prob=0.260)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): DownBlock(
      (mlp): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768, bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (skip): Sequential(
        (0): ConvX(
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
        (1): ConvX(
          (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (drop_path): DropPath(drop_prob=0.280)
    )
    (1): BottleNeck(
      (drop_path): DropPath(drop_prob=0.300)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (2): BottleNeck(
      (drop_path): DropPath(drop_prob=0.320)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (3): BottleNeck(
      (drop_path): DropPath(drop_prob=0.340)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (4): BottleNeck(
      (drop_path): DropPath(drop_prob=0.360)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (5): BottleNeck(
      (drop_path): DropPath(drop_prob=0.380)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (6): BottleNeck(
      (drop_path): DropPath(drop_prob=0.400)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (7): BottleNeck(
      (drop_path): DropPath(drop_prob=0.420)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (8): BottleNeck(
      (drop_path): DropPath(drop_prob=0.440)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (9): BottleNeck(
      (drop_path): DropPath(drop_prob=0.460)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (10): BottleNeck(
      (drop_path): DropPath(drop_prob=0.480)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (11): BottleNeck(
      (drop_path): DropPath(drop_prob=0.500)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (12): BottleNeck(
      (drop_path): DropPath(drop_prob=0.520)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (13): BottleNeck(
      (drop_path): DropPath(drop_prob=0.540)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (14): BottleNeck(
      (drop_path): DropPath(drop_prob=0.560)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (15): BottleNeck(
      (drop_path): DropPath(drop_prob=0.580)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (16): BottleNeck(
      (drop_path): DropPath(drop_prob=0.600)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): DownBlock(
      (mlp): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): ConvX(
          (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1536, bias=False)
          (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (skip): Sequential(
        (0): ConvX(
          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
        (1): ConvX(
          (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
      (drop_path): DropPath(drop_prob=0.620)
    )
    (1): BottleNeck(
      (drop_path): DropPath(drop_prob=0.640)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (2): BottleNeck(
      (drop_path): DropPath(drop_prob=0.660)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (3): BottleNeck(
      (drop_path): DropPath(drop_prob=0.680)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
    (4): BottleNeck(
      (drop_path): DropPath(drop_prob=0.700)
      (ln): LayerNorm()
      (sblock_in): ConvX(
        (conv): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act(
          (act): GELU(approximate='none')
        )
      )
      (sblock_dw): ConvX(
        (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
        (norm): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (sblock_proj): ConvX(
        (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Act()
      )
      (mblock): Sequential(
        (0): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (1): PyramidConvX(
          (branch_1): Sequential(
            (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_2): Sequential(
            (0): AvgPool2d(kernel_size=5, stride=2, padding=2)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (branch_3): Sequential(
            (0): AvgPool2d(kernel_size=7, stride=3, padding=3)
            (1): ConvX(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Act()
            )
          )
          (act): Act(
            (act): GELU(approximate='none')
          )
        )
        (2): ConvX(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Act()
        )
      )
    )
  )
  (head): ConvX(
    (conv): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): Act(
      (act): GELU(approximate='none')
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (classifier): MlpHead(
    (fc1): Linear(in_features=1024, out_features=2048, bias=False)
    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act): Act(
      (act): GELU(approximate='none')
    )
    (drop): Dropout(p=0.2, inplace=False)
    (fc2): Linear(in_features=2048, out_features=1000, bias=False)
  )
)
number of params: 48903328
LR = 0.00005000
Batch size = 1024
Update frequent = 4
Number of training examples = 1281167
Number of training training per epoch = 1251
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "first_conv.conv.weight",
      "layer1.0.mlp.0.conv.weight",
      "layer1.0.mlp.1.conv.weight",
      "layer1.0.mlp.2.conv.weight",
      "layer1.0.skip.0.conv.weight",
      "layer1.0.skip.1.conv.weight",
      "layer1.1.sblock_in.conv.weight",
      "layer1.1.sblock_dw.conv.weight",
      "layer1.1.sblock_proj.conv.weight",
      "layer1.1.mblock.0.conv.weight",
      "layer1.1.mblock.1.branch_1.1.conv.weight",
      "layer1.1.mblock.1.branch_2.1.conv.weight",
      "layer1.1.mblock.1.branch_3.1.conv.weight",
      "layer1.1.mblock.2.conv.weight",
      "layer1.2.sblock_in.conv.weight",
      "layer1.2.sblock_dw.conv.weight",
      "layer1.2.sblock_proj.conv.weight",
      "layer1.2.mblock.0.conv.weight",
      "layer1.2.mblock.1.branch_1.1.conv.weight",
      "layer1.2.mblock.1.branch_2.1.conv.weight",
      "layer1.2.mblock.1.branch_3.1.conv.weight",
      "layer1.2.mblock.2.conv.weight",
      "layer1.3.sblock_in.conv.weight",
      "layer1.3.sblock_dw.conv.weight",
      "layer1.3.sblock_proj.conv.weight",
      "layer1.3.mblock.0.conv.weight",
      "layer1.3.mblock.1.branch_1.1.conv.weight",
      "layer1.3.mblock.1.branch_2.1.conv.weight",
      "layer1.3.mblock.1.branch_3.1.conv.weight",
      "layer1.3.mblock.2.conv.weight",
      "layer1.4.sblock_in.conv.weight",
      "layer1.4.sblock_dw.conv.weight",
      "layer1.4.sblock_proj.conv.weight",
      "layer1.4.mblock.0.conv.weight",
      "layer1.4.mblock.1.branch_1.1.conv.weight",
      "layer1.4.mblock.1.branch_2.1.conv.weight",
      "layer1.4.mblock.1.branch_3.1.conv.weight",
      "layer1.4.mblock.2.conv.weight",
      "layer2.0.mlp.0.conv.weight",
      "layer2.0.mlp.1.conv.weight",
      "layer2.0.mlp.2.conv.weight",
      "layer2.0.skip.0.conv.weight",
      "layer2.0.skip.1.conv.weight",
      "layer2.1.sblock_in.conv.weight",
      "layer2.1.sblock_dw.conv.weight",
      "layer2.1.sblock_proj.conv.weight",
      "layer2.1.mblock.0.conv.weight",
      "layer2.1.mblock.1.branch_1.1.conv.weight",
      "layer2.1.mblock.1.branch_2.1.conv.weight",
      "layer2.1.mblock.1.branch_3.1.conv.weight",
      "layer2.1.mblock.2.conv.weight",
      "layer2.2.sblock_in.conv.weight",
      "layer2.2.sblock_dw.conv.weight",
      "layer2.2.sblock_proj.conv.weight",
      "layer2.2.mblock.0.conv.weight",
      "layer2.2.mblock.1.branch_1.1.conv.weight",
      "layer2.2.mblock.1.branch_2.1.conv.weight",
      "layer2.2.mblock.1.branch_3.1.conv.weight",
      "layer2.2.mblock.2.conv.weight",
      "layer2.3.sblock_in.conv.weight",
      "layer2.3.sblock_dw.conv.weight",
      "layer2.3.sblock_proj.conv.weight",
      "layer2.3.mblock.0.conv.weight",
      "layer2.3.mblock.1.branch_1.1.conv.weight",
      "layer2.3.mblock.1.branch_2.1.conv.weight",
      "layer2.3.mblock.1.branch_3.1.conv.weight",
      "layer2.3.mblock.2.conv.weight",
      "layer2.4.sblock_in.conv.weight",
      "layer2.4.sblock_dw.conv.weight",
      "layer2.4.sblock_proj.conv.weight",
      "layer2.4.mblock.0.conv.weight",
      "layer2.4.mblock.1.branch_1.1.conv.weight",
      "layer2.4.mblock.1.branch_2.1.conv.weight",
      "layer2.4.mblock.1.branch_3.1.conv.weight",
      "layer2.4.mblock.2.conv.weight",
      "layer2.5.sblock_in.conv.weight",
      "layer2.5.sblock_dw.conv.weight",
      "layer2.5.sblock_proj.conv.weight",
      "layer2.5.mblock.0.conv.weight",
      "layer2.5.mblock.1.branch_1.1.conv.weight",
      "layer2.5.mblock.1.branch_2.1.conv.weight",
      "layer2.5.mblock.1.branch_3.1.conv.weight",
      "layer2.5.mblock.2.conv.weight",
      "layer2.6.sblock_in.conv.weight",
      "layer2.6.sblock_dw.conv.weight",
      "layer2.6.sblock_proj.conv.weight",
      "layer2.6.mblock.0.conv.weight",
      "layer2.6.mblock.1.branch_1.1.conv.weight",
      "layer2.6.mblock.1.branch_2.1.conv.weight",
      "layer2.6.mblock.1.branch_3.1.conv.weight",
      "layer2.6.mblock.2.conv.weight",
      "layer2.7.sblock_in.conv.weight",
      "layer2.7.sblock_dw.conv.weight",
      "layer2.7.sblock_proj.conv.weight",
      "layer2.7.mblock.0.conv.weight",
      "layer2.7.mblock.1.branch_1.1.conv.weight",
      "layer2.7.mblock.1.branch_2.1.conv.weight",
      "layer2.7.mblock.1.branch_3.1.conv.weight",
      "layer2.7.mblock.2.conv.weight",
      "layer2.8.sblock_in.conv.weight",
      "layer2.8.sblock_dw.conv.weight",
      "layer2.8.sblock_proj.conv.weight",
      "layer2.8.mblock.0.conv.weight",
      "layer2.8.mblock.1.branch_1.1.conv.weight",
      "layer2.8.mblock.1.branch_2.1.conv.weight",
      "layer2.8.mblock.1.branch_3.1.conv.weight",
      "layer2.8.mblock.2.conv.weight",
      "layer3.0.mlp.0.conv.weight",
      "layer3.0.mlp.1.conv.weight",
      "layer3.0.mlp.2.conv.weight",
      "layer3.0.skip.0.conv.weight",
      "layer3.0.skip.1.conv.weight",
      "layer3.1.sblock_in.conv.weight",
      "layer3.1.sblock_dw.conv.weight",
      "layer3.1.sblock_proj.conv.weight",
      "layer3.1.mblock.0.conv.weight",
      "layer3.1.mblock.1.branch_1.1.conv.weight",
      "layer3.1.mblock.1.branch_2.1.conv.weight",
      "layer3.1.mblock.1.branch_3.1.conv.weight",
      "layer3.1.mblock.2.conv.weight",
      "layer3.2.sblock_in.conv.weight",
      "layer3.2.sblock_dw.conv.weight",
      "layer3.2.sblock_proj.conv.weight",
      "layer3.2.mblock.0.conv.weight",
      "layer3.2.mblock.1.branch_1.1.conv.weight",
      "layer3.2.mblock.1.branch_2.1.conv.weight",
      "layer3.2.mblock.1.branch_3.1.conv.weight",
      "layer3.2.mblock.2.conv.weight",
      "layer3.3.sblock_in.conv.weight",
      "layer3.3.sblock_dw.conv.weight",
      "layer3.3.sblock_proj.conv.weight",
      "layer3.3.mblock.0.conv.weight",
      "layer3.3.mblock.1.branch_1.1.conv.weight",
      "layer3.3.mblock.1.branch_2.1.conv.weight",
      "layer3.3.mblock.1.branch_3.1.conv.weight",
      "layer3.3.mblock.2.conv.weight",
      "layer3.4.sblock_in.conv.weight",
      "layer3.4.sblock_dw.conv.weight",
      "layer3.4.sblock_proj.conv.weight",
      "layer3.4.mblock.0.conv.weight",
      "layer3.4.mblock.1.branch_1.1.conv.weight",
      "layer3.4.mblock.1.branch_2.1.conv.weight",
      "layer3.4.mblock.1.branch_3.1.conv.weight",
      "layer3.4.mblock.2.conv.weight",
      "layer3.5.sblock_in.conv.weight",
      "layer3.5.sblock_dw.conv.weight",
      "layer3.5.sblock_proj.conv.weight",
      "layer3.5.mblock.0.conv.weight",
      "layer3.5.mblock.1.branch_1.1.conv.weight",
      "layer3.5.mblock.1.branch_2.1.conv.weight",
      "layer3.5.mblock.1.branch_3.1.conv.weight",
      "layer3.5.mblock.2.conv.weight",
      "layer3.6.sblock_in.conv.weight",
      "layer3.6.sblock_dw.conv.weight",
      "layer3.6.sblock_proj.conv.weight",
      "layer3.6.mblock.0.conv.weight",
      "layer3.6.mblock.1.branch_1.1.conv.weight",
      "layer3.6.mblock.1.branch_2.1.conv.weight",
      "layer3.6.mblock.1.branch_3.1.conv.weight",
      "layer3.6.mblock.2.conv.weight",
      "layer3.7.sblock_in.conv.weight",
      "layer3.7.sblock_dw.conv.weight",
      "layer3.7.sblock_proj.conv.weight",
      "layer3.7.mblock.0.conv.weight",
      "layer3.7.mblock.1.branch_1.1.conv.weight",
      "layer3.7.mblock.1.branch_2.1.conv.weight",
      "layer3.7.mblock.1.branch_3.1.conv.weight",
      "layer3.7.mblock.2.conv.weight",
      "layer3.8.sblock_in.conv.weight",
      "layer3.8.sblock_dw.conv.weight",
      "layer3.8.sblock_proj.conv.weight",
      "layer3.8.mblock.0.conv.weight",
      "layer3.8.mblock.1.branch_1.1.conv.weight",
      "layer3.8.mblock.1.branch_2.1.conv.weight",
      "layer3.8.mblock.1.branch_3.1.conv.weight",
      "layer3.8.mblock.2.conv.weight",
      "layer3.9.sblock_in.conv.weight",
      "layer3.9.sblock_dw.conv.weight",
      "layer3.9.sblock_proj.conv.weight",
      "layer3.9.mblock.0.conv.weight",
      "layer3.9.mblock.1.branch_1.1.conv.weight",
      "layer3.9.mblock.1.branch_2.1.conv.weight",
      "layer3.9.mblock.1.branch_3.1.conv.weight",
      "layer3.9.mblock.2.conv.weight",
      "layer3.10.sblock_in.conv.weight",
      "layer3.10.sblock_dw.conv.weight",
      "layer3.10.sblock_proj.conv.weight",
      "layer3.10.mblock.0.conv.weight",
      "layer3.10.mblock.1.branch_1.1.conv.weight",
      "layer3.10.mblock.1.branch_2.1.conv.weight",
      "layer3.10.mblock.1.branch_3.1.conv.weight",
      "layer3.10.mblock.2.conv.weight",
      "layer3.11.sblock_in.conv.weight",
      "layer3.11.sblock_dw.conv.weight",
      "layer3.11.sblock_proj.conv.weight",
      "layer3.11.mblock.0.conv.weight",
      "layer3.11.mblock.1.branch_1.1.conv.weight",
      "layer3.11.mblock.1.branch_2.1.conv.weight",
      "layer3.11.mblock.1.branch_3.1.conv.weight",
      "layer3.11.mblock.2.conv.weight",
      "layer3.12.sblock_in.conv.weight",
      "layer3.12.sblock_dw.conv.weight",
      "layer3.12.sblock_proj.conv.weight",
      "layer3.12.mblock.0.conv.weight",
      "layer3.12.mblock.1.branch_1.1.conv.weight",
      "layer3.12.mblock.1.branch_2.1.conv.weight",
      "layer3.12.mblock.1.branch_3.1.conv.weight",
      "layer3.12.mblock.2.conv.weight",
      "layer3.13.sblock_in.conv.weight",
      "layer3.13.sblock_dw.conv.weight",
      "layer3.13.sblock_proj.conv.weight",
      "layer3.13.mblock.0.conv.weight",
      "layer3.13.mblock.1.branch_1.1.conv.weight",
      "layer3.13.mblock.1.branch_2.1.conv.weight",
      "layer3.13.mblock.1.branch_3.1.conv.weight",
      "layer3.13.mblock.2.conv.weight",
      "layer3.14.sblock_in.conv.weight",
      "layer3.14.sblock_dw.conv.weight",
      "layer3.14.sblock_proj.conv.weight",
      "layer3.14.mblock.0.conv.weight",
      "layer3.14.mblock.1.branch_1.1.conv.weight",
      "layer3.14.mblock.1.branch_2.1.conv.weight",
      "layer3.14.mblock.1.branch_3.1.conv.weight",
      "layer3.14.mblock.2.conv.weight",
      "layer3.15.sblock_in.conv.weight",
      "layer3.15.sblock_dw.conv.weight",
      "layer3.15.sblock_proj.conv.weight",
      "layer3.15.mblock.0.conv.weight",
      "layer3.15.mblock.1.branch_1.1.conv.weight",
      "layer3.15.mblock.1.branch_2.1.conv.weight",
      "layer3.15.mblock.1.branch_3.1.conv.weight",
      "layer3.15.mblock.2.conv.weight",
      "layer3.16.sblock_in.conv.weight",
      "layer3.16.sblock_dw.conv.weight",
      "layer3.16.sblock_proj.conv.weight",
      "layer3.16.mblock.0.conv.weight",
      "layer3.16.mblock.1.branch_1.1.conv.weight",
      "layer3.16.mblock.1.branch_2.1.conv.weight",
      "layer3.16.mblock.1.branch_3.1.conv.weight",
      "layer3.16.mblock.2.conv.weight",
      "layer4.0.mlp.0.conv.weight",
      "layer4.0.mlp.1.conv.weight",
      "layer4.0.mlp.2.conv.weight",
      "layer4.0.skip.0.conv.weight",
      "layer4.0.skip.1.conv.weight",
      "layer4.1.sblock_in.conv.weight",
      "layer4.1.sblock_dw.conv.weight",
      "layer4.1.sblock_proj.conv.weight",
      "layer4.1.mblock.0.conv.weight",
      "layer4.1.mblock.1.branch_1.1.conv.weight",
      "layer4.1.mblock.1.branch_2.1.conv.weight",
      "layer4.1.mblock.1.branch_3.1.conv.weight",
      "layer4.1.mblock.2.conv.weight",
      "layer4.2.sblock_in.conv.weight",
      "layer4.2.sblock_dw.conv.weight",
      "layer4.2.sblock_proj.conv.weight",
      "layer4.2.mblock.0.conv.weight",
      "layer4.2.mblock.1.branch_1.1.conv.weight",
      "layer4.2.mblock.1.branch_2.1.conv.weight",
      "layer4.2.mblock.1.branch_3.1.conv.weight",
      "layer4.2.mblock.2.conv.weight",
      "layer4.3.sblock_in.conv.weight",
      "layer4.3.sblock_dw.conv.weight",
      "layer4.3.sblock_proj.conv.weight",
      "layer4.3.mblock.0.conv.weight",
      "layer4.3.mblock.1.branch_1.1.conv.weight",
      "layer4.3.mblock.1.branch_2.1.conv.weight",
      "layer4.3.mblock.1.branch_3.1.conv.weight",
      "layer4.3.mblock.2.conv.weight",
      "layer4.4.sblock_in.conv.weight",
      "layer4.4.sblock_dw.conv.weight",
      "layer4.4.sblock_proj.conv.weight",
      "layer4.4.mblock.0.conv.weight",
      "layer4.4.mblock.1.branch_1.1.conv.weight",
      "layer4.4.mblock.1.branch_2.1.conv.weight",
      "layer4.4.mblock.1.branch_3.1.conv.weight",
      "layer4.4.mblock.2.conv.weight",
      "head.conv.weight",
      "classifier.fc1.weight",
      "classifier.fc2.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "first_conv.norm.weight",
      "first_conv.norm.bias",
      "layer1.0.mlp.0.norm.weight",
      "layer1.0.mlp.0.norm.bias",
      "layer1.0.mlp.1.norm.weight",
      "layer1.0.mlp.1.norm.bias",
      "layer1.0.mlp.2.norm.weight",
      "layer1.0.mlp.2.norm.bias",
      "layer1.0.skip.0.norm.weight",
      "layer1.0.skip.0.norm.bias",
      "layer1.0.skip.1.norm.weight",
      "layer1.0.skip.1.norm.bias",
      "layer1.1.sblock_in.norm.weight",
      "layer1.1.sblock_in.norm.bias",
      "layer1.1.sblock_dw.norm.weight",
      "layer1.1.sblock_dw.norm.bias",
      "layer1.1.sblock_proj.norm.weight",
      "layer1.1.sblock_proj.norm.bias",
      "layer1.1.mblock.0.norm.weight",
      "layer1.1.mblock.0.norm.bias",
      "layer1.1.mblock.1.branch_1.1.norm.weight",
      "layer1.1.mblock.1.branch_1.1.norm.bias",
      "layer1.1.mblock.1.branch_2.1.norm.weight",
      "layer1.1.mblock.1.branch_2.1.norm.bias",
      "layer1.1.mblock.1.branch_3.1.norm.weight",
      "layer1.1.mblock.1.branch_3.1.norm.bias",
      "layer1.1.mblock.2.norm.weight",
      "layer1.1.mblock.2.norm.bias",
      "layer1.2.sblock_in.norm.weight",
      "layer1.2.sblock_in.norm.bias",
      "layer1.2.sblock_dw.norm.weight",
      "layer1.2.sblock_dw.norm.bias",
      "layer1.2.sblock_proj.norm.weight",
      "layer1.2.sblock_proj.norm.bias",
      "layer1.2.mblock.0.norm.weight",
      "layer1.2.mblock.0.norm.bias",
      "layer1.2.mblock.1.branch_1.1.norm.weight",
      "layer1.2.mblock.1.branch_1.1.norm.bias",
      "layer1.2.mblock.1.branch_2.1.norm.weight",
      "layer1.2.mblock.1.branch_2.1.norm.bias",
      "layer1.2.mblock.1.branch_3.1.norm.weight",
      "layer1.2.mblock.1.branch_3.1.norm.bias",
      "layer1.2.mblock.2.norm.weight",
      "layer1.2.mblock.2.norm.bias",
      "layer1.3.sblock_in.norm.weight",
      "layer1.3.sblock_in.norm.bias",
      "layer1.3.sblock_dw.norm.weight",
      "layer1.3.sblock_dw.norm.bias",
      "layer1.3.sblock_proj.norm.weight",
      "layer1.3.sblock_proj.norm.bias",
      "layer1.3.mblock.0.norm.weight",
      "layer1.3.mblock.0.norm.bias",
      "layer1.3.mblock.1.branch_1.1.norm.weight",
      "layer1.3.mblock.1.branch_1.1.norm.bias",
      "layer1.3.mblock.1.branch_2.1.norm.weight",
      "layer1.3.mblock.1.branch_2.1.norm.bias",
      "layer1.3.mblock.1.branch_3.1.norm.weight",
      "layer1.3.mblock.1.branch_3.1.norm.bias",
      "layer1.3.mblock.2.norm.weight",
      "layer1.3.mblock.2.norm.bias",
      "layer1.4.sblock_in.norm.weight",
      "layer1.4.sblock_in.norm.bias",
      "layer1.4.sblock_dw.norm.weight",
      "layer1.4.sblock_dw.norm.bias",
      "layer1.4.sblock_proj.norm.weight",
      "layer1.4.sblock_proj.norm.bias",
      "layer1.4.mblock.0.norm.weight",
      "layer1.4.mblock.0.norm.bias",
      "layer1.4.mblock.1.branch_1.1.norm.weight",
      "layer1.4.mblock.1.branch_1.1.norm.bias",
      "layer1.4.mblock.1.branch_2.1.norm.weight",
      "layer1.4.mblock.1.branch_2.1.norm.bias",
      "layer1.4.mblock.1.branch_3.1.norm.weight",
      "layer1.4.mblock.1.branch_3.1.norm.bias",
      "layer1.4.mblock.2.norm.weight",
      "layer1.4.mblock.2.norm.bias",
      "layer2.0.mlp.0.norm.weight",
      "layer2.0.mlp.0.norm.bias",
      "layer2.0.mlp.1.norm.weight",
      "layer2.0.mlp.1.norm.bias",
      "layer2.0.mlp.2.norm.weight",
      "layer2.0.mlp.2.norm.bias",
      "layer2.0.skip.0.norm.weight",
      "layer2.0.skip.0.norm.bias",
      "layer2.0.skip.1.norm.weight",
      "layer2.0.skip.1.norm.bias",
      "layer2.1.sblock_in.norm.weight",
      "layer2.1.sblock_in.norm.bias",
      "layer2.1.sblock_dw.norm.weight",
      "layer2.1.sblock_dw.norm.bias",
      "layer2.1.sblock_proj.norm.weight",
      "layer2.1.sblock_proj.norm.bias",
      "layer2.1.mblock.0.norm.weight",
      "layer2.1.mblock.0.norm.bias",
      "layer2.1.mblock.1.branch_1.1.norm.weight",
      "layer2.1.mblock.1.branch_1.1.norm.bias",
      "layer2.1.mblock.1.branch_2.1.norm.weight",
      "layer2.1.mblock.1.branch_2.1.norm.bias",
      "layer2.1.mblock.1.branch_3.1.norm.weight",
      "layer2.1.mblock.1.branch_3.1.norm.bias",
      "layer2.1.mblock.2.norm.weight",
      "layer2.1.mblock.2.norm.bias",
      "layer2.2.sblock_in.norm.weight",
      "layer2.2.sblock_in.norm.bias",
      "layer2.2.sblock_dw.norm.weight",
      "layer2.2.sblock_dw.norm.bias",
      "layer2.2.sblock_proj.norm.weight",
      "layer2.2.sblock_proj.norm.bias",
      "layer2.2.mblock.0.norm.weight",
      "layer2.2.mblock.0.norm.bias",
      "layer2.2.mblock.1.branch_1.1.norm.weight",
      "layer2.2.mblock.1.branch_1.1.norm.bias",
      "layer2.2.mblock.1.branch_2.1.norm.weight",
      "layer2.2.mblock.1.branch_2.1.norm.bias",
      "layer2.2.mblock.1.branch_3.1.norm.weight",
      "layer2.2.mblock.1.branch_3.1.norm.bias",
      "layer2.2.mblock.2.norm.weight",
      "layer2.2.mblock.2.norm.bias",
      "layer2.3.sblock_in.norm.weight",
      "layer2.3.sblock_in.norm.bias",
      "layer2.3.sblock_dw.norm.weight",
      "layer2.3.sblock_dw.norm.bias",
      "layer2.3.sblock_proj.norm.weight",
      "layer2.3.sblock_proj.norm.bias",
      "layer2.3.mblock.0.norm.weight",
      "layer2.3.mblock.0.norm.bias",
      "layer2.3.mblock.1.branch_1.1.norm.weight",
      "layer2.3.mblock.1.branch_1.1.norm.bias",
      "layer2.3.mblock.1.branch_2.1.norm.weight",
      "layer2.3.mblock.1.branch_2.1.norm.bias",
      "layer2.3.mblock.1.branch_3.1.norm.weight",
      "layer2.3.mblock.1.branch_3.1.norm.bias",
      "layer2.3.mblock.2.norm.weight",
      "layer2.3.mblock.2.norm.bias",
      "layer2.4.sblock_in.norm.weight",
      "layer2.4.sblock_in.norm.bias",
      "layer2.4.sblock_dw.norm.weight",
      "layer2.4.sblock_dw.norm.bias",
      "layer2.4.sblock_proj.norm.weight",
      "layer2.4.sblock_proj.norm.bias",
      "layer2.4.mblock.0.norm.weight",
      "layer2.4.mblock.0.norm.bias",
      "layer2.4.mblock.1.branch_1.1.norm.weight",
      "layer2.4.mblock.1.branch_1.1.norm.bias",
      "layer2.4.mblock.1.branch_2.1.norm.weight",
      "layer2.4.mblock.1.branch_2.1.norm.bias",
      "layer2.4.mblock.1.branch_3.1.norm.weight",
      "layer2.4.mblock.1.branch_3.1.norm.bias",
      "layer2.4.mblock.2.norm.weight",
      "layer2.4.mblock.2.norm.bias",
      "layer2.5.sblock_in.norm.weight",
      "layer2.5.sblock_in.norm.bias",
      "layer2.5.sblock_dw.norm.weight",
      "layer2.5.sblock_dw.norm.bias",
      "layer2.5.sblock_proj.norm.weight",
      "layer2.5.sblock_proj.norm.bias",
      "layer2.5.mblock.0.norm.weight",
      "layer2.5.mblock.0.norm.bias",
      "layer2.5.mblock.1.branch_1.1.norm.weight",
      "layer2.5.mblock.1.branch_1.1.norm.bias",
      "layer2.5.mblock.1.branch_2.1.norm.weight",
      "layer2.5.mblock.1.branch_2.1.norm.bias",
      "layer2.5.mblock.1.branch_3.1.norm.weight",
      "layer2.5.mblock.1.branch_3.1.norm.bias",
      "layer2.5.mblock.2.norm.weight",
      "layer2.5.mblock.2.norm.bias",
      "layer2.6.sblock_in.norm.weight",
      "layer2.6.sblock_in.norm.bias",
      "layer2.6.sblock_dw.norm.weight",
      "layer2.6.sblock_dw.norm.bias",
      "layer2.6.sblock_proj.norm.weight",
      "layer2.6.sblock_proj.norm.bias",
      "layer2.6.mblock.0.norm.weight",
      "layer2.6.mblock.0.norm.bias",
      "layer2.6.mblock.1.branch_1.1.norm.weight",
      "layer2.6.mblock.1.branch_1.1.norm.bias",
      "layer2.6.mblock.1.branch_2.1.norm.weight",
      "layer2.6.mblock.1.branch_2.1.norm.bias",
      "layer2.6.mblock.1.branch_3.1.norm.weight",
      "layer2.6.mblock.1.branch_3.1.norm.bias",
      "layer2.6.mblock.2.norm.weight",
      "layer2.6.mblock.2.norm.bias",
      "layer2.7.sblock_in.norm.weight",
      "layer2.7.sblock_in.norm.bias",
      "layer2.7.sblock_dw.norm.weight",
      "layer2.7.sblock_dw.norm.bias",
      "layer2.7.sblock_proj.norm.weight",
      "layer2.7.sblock_proj.norm.bias",
      "layer2.7.mblock.0.norm.weight",
      "layer2.7.mblock.0.norm.bias",
      "layer2.7.mblock.1.branch_1.1.norm.weight",
      "layer2.7.mblock.1.branch_1.1.norm.bias",
      "layer2.7.mblock.1.branch_2.1.norm.weight",
      "layer2.7.mblock.1.branch_2.1.norm.bias",
      "layer2.7.mblock.1.branch_3.1.norm.weight",
      "layer2.7.mblock.1.branch_3.1.norm.bias",
      "layer2.7.mblock.2.norm.weight",
      "layer2.7.mblock.2.norm.bias",
      "layer2.8.sblock_in.norm.weight",
      "layer2.8.sblock_in.norm.bias",
      "layer2.8.sblock_dw.norm.weight",
      "layer2.8.sblock_dw.norm.bias",
      "layer2.8.sblock_proj.norm.weight",
      "layer2.8.sblock_proj.norm.bias",
      "layer2.8.mblock.0.norm.weight",
      "layer2.8.mblock.0.norm.bias",
      "layer2.8.mblock.1.branch_1.1.norm.weight",
      "layer2.8.mblock.1.branch_1.1.norm.bias",
      "layer2.8.mblock.1.branch_2.1.norm.weight",
      "layer2.8.mblock.1.branch_2.1.norm.bias",
      "layer2.8.mblock.1.branch_3.1.norm.weight",
      "layer2.8.mblock.1.branch_3.1.norm.bias",
      "layer2.8.mblock.2.norm.weight",
      "layer2.8.mblock.2.norm.bias",
      "layer3.0.mlp.0.norm.weight",
      "layer3.0.mlp.0.norm.bias",
      "layer3.0.mlp.1.norm.weight",
      "layer3.0.mlp.1.norm.bias",
      "layer3.0.mlp.2.norm.weight",
      "layer3.0.mlp.2.norm.bias",
      "layer3.0.skip.0.norm.weight",
      "layer3.0.skip.0.norm.bias",
      "layer3.0.skip.1.norm.weight",
      "layer3.0.skip.1.norm.bias",
      "layer3.1.sblock_in.norm.weight",
      "layer3.1.sblock_in.norm.bias",
      "layer3.1.sblock_dw.norm.weight",
      "layer3.1.sblock_dw.norm.bias",
      "layer3.1.sblock_proj.norm.weight",
      "layer3.1.sblock_proj.norm.bias",
      "layer3.1.mblock.0.norm.weight",
      "layer3.1.mblock.0.norm.bias",
      "layer3.1.mblock.1.branch_1.1.norm.weight",
      "layer3.1.mblock.1.branch_1.1.norm.bias",
      "layer3.1.mblock.1.branch_2.1.norm.weight",
      "layer3.1.mblock.1.branch_2.1.norm.bias",
      "layer3.1.mblock.1.branch_3.1.norm.weight",
      "layer3.1.mblock.1.branch_3.1.norm.bias",
      "layer3.1.mblock.2.norm.weight",
      "layer3.1.mblock.2.norm.bias",
      "layer3.2.sblock_in.norm.weight",
      "layer3.2.sblock_in.norm.bias",
      "layer3.2.sblock_dw.norm.weight",
      "layer3.2.sblock_dw.norm.bias",
      "layer3.2.sblock_proj.norm.weight",
      "layer3.2.sblock_proj.norm.bias",
      "layer3.2.mblock.0.norm.weight",
      "layer3.2.mblock.0.norm.bias",
      "layer3.2.mblock.1.branch_1.1.norm.weight",
      "layer3.2.mblock.1.branch_1.1.norm.bias",
      "layer3.2.mblock.1.branch_2.1.norm.weight",
      "layer3.2.mblock.1.branch_2.1.norm.bias",
      "layer3.2.mblock.1.branch_3.1.norm.weight",
      "layer3.2.mblock.1.branch_3.1.norm.bias",
      "layer3.2.mblock.2.norm.weight",
      "layer3.2.mblock.2.norm.bias",
      "layer3.3.sblock_in.norm.weight",
      "layer3.3.sblock_in.norm.bias",
      "layer3.3.sblock_dw.norm.weight",
      "layer3.3.sblock_dw.norm.bias",
      "layer3.3.sblock_proj.norm.weight",
      "layer3.3.sblock_proj.norm.bias",
      "layer3.3.mblock.0.norm.weight",
      "layer3.3.mblock.0.norm.bias",
      "layer3.3.mblock.1.branch_1.1.norm.weight",
      "layer3.3.mblock.1.branch_1.1.norm.bias",
      "layer3.3.mblock.1.branch_2.1.norm.weight",
      "layer3.3.mblock.1.branch_2.1.norm.bias",
      "layer3.3.mblock.1.branch_3.1.norm.weight",
      "layer3.3.mblock.1.branch_3.1.norm.bias",
      "layer3.3.mblock.2.norm.weight",
      "layer3.3.mblock.2.norm.bias",
      "layer3.4.sblock_in.norm.weight",
      "layer3.4.sblock_in.norm.bias",
      "layer3.4.sblock_dw.norm.weight",
      "layer3.4.sblock_dw.norm.bias",
      "layer3.4.sblock_proj.norm.weight",
      "layer3.4.sblock_proj.norm.bias",
      "layer3.4.mblock.0.norm.weight",
      "layer3.4.mblock.0.norm.bias",
      "layer3.4.mblock.1.branch_1.1.norm.weight",
      "layer3.4.mblock.1.branch_1.1.norm.bias",
      "layer3.4.mblock.1.branch_2.1.norm.weight",
      "layer3.4.mblock.1.branch_2.1.norm.bias",
      "layer3.4.mblock.1.branch_3.1.norm.weight",
      "layer3.4.mblock.1.branch_3.1.norm.bias",
      "layer3.4.mblock.2.norm.weight",
      "layer3.4.mblock.2.norm.bias",
      "layer3.5.sblock_in.norm.weight",
      "layer3.5.sblock_in.norm.bias",
      "layer3.5.sblock_dw.norm.weight",
      "layer3.5.sblock_dw.norm.bias",
      "layer3.5.sblock_proj.norm.weight",
      "layer3.5.sblock_proj.norm.bias",
      "layer3.5.mblock.0.norm.weight",
      "layer3.5.mblock.0.norm.bias",
      "layer3.5.mblock.1.branch_1.1.norm.weight",
      "layer3.5.mblock.1.branch_1.1.norm.bias",
      "layer3.5.mblock.1.branch_2.1.norm.weight",
      "layer3.5.mblock.1.branch_2.1.norm.bias",
      "layer3.5.mblock.1.branch_3.1.norm.weight",
      "layer3.5.mblock.1.branch_3.1.norm.bias",
      "layer3.5.mblock.2.norm.weight",
      "layer3.5.mblock.2.norm.bias",
      "layer3.6.sblock_in.norm.weight",
      "layer3.6.sblock_in.norm.bias",
      "layer3.6.sblock_dw.norm.weight",
      "layer3.6.sblock_dw.norm.bias",
      "layer3.6.sblock_proj.norm.weight",
      "layer3.6.sblock_proj.norm.bias",
      "layer3.6.mblock.0.norm.weight",
      "layer3.6.mblock.0.norm.bias",
      "layer3.6.mblock.1.branch_1.1.norm.weight",
      "layer3.6.mblock.1.branch_1.1.norm.bias",
      "layer3.6.mblock.1.branch_2.1.norm.weight",
      "layer3.6.mblock.1.branch_2.1.norm.bias",
      "layer3.6.mblock.1.branch_3.1.norm.weight",
      "layer3.6.mblock.1.branch_3.1.norm.bias",
      "layer3.6.mblock.2.norm.weight",
      "layer3.6.mblock.2.norm.bias",
      "layer3.7.sblock_in.norm.weight",
      "layer3.7.sblock_in.norm.bias",
      "layer3.7.sblock_dw.norm.weight",
      "layer3.7.sblock_dw.norm.bias",
      "layer3.7.sblock_proj.norm.weight",
      "layer3.7.sblock_proj.norm.bias",
      "layer3.7.mblock.0.norm.weight",
      "layer3.7.mblock.0.norm.bias",
      "layer3.7.mblock.1.branch_1.1.norm.weight",
      "layer3.7.mblock.1.branch_1.1.norm.bias",
      "layer3.7.mblock.1.branch_2.1.norm.weight",
      "layer3.7.mblock.1.branch_2.1.norm.bias",
      "layer3.7.mblock.1.branch_3.1.norm.weight",
      "layer3.7.mblock.1.branch_3.1.norm.bias",
      "layer3.7.mblock.2.norm.weight",
      "layer3.7.mblock.2.norm.bias",
      "layer3.8.sblock_in.norm.weight",
      "layer3.8.sblock_in.norm.bias",
      "layer3.8.sblock_dw.norm.weight",
      "layer3.8.sblock_dw.norm.bias",
      "layer3.8.sblock_proj.norm.weight",
      "layer3.8.sblock_proj.norm.bias",
      "layer3.8.mblock.0.norm.weight",
      "layer3.8.mblock.0.norm.bias",
      "layer3.8.mblock.1.branch_1.1.norm.weight",
      "layer3.8.mblock.1.branch_1.1.norm.bias",
      "layer3.8.mblock.1.branch_2.1.norm.weight",
      "layer3.8.mblock.1.branch_2.1.norm.bias",
      "layer3.8.mblock.1.branch_3.1.norm.weight",
      "layer3.8.mblock.1.branch_3.1.norm.bias",
      "layer3.8.mblock.2.norm.weight",
      "layer3.8.mblock.2.norm.bias",
      "layer3.9.sblock_in.norm.weight",
      "layer3.9.sblock_in.norm.bias",
      "layer3.9.sblock_dw.norm.weight",
      "layer3.9.sblock_dw.norm.bias",
      "layer3.9.sblock_proj.norm.weight",
      "layer3.9.sblock_proj.norm.bias",
      "layer3.9.mblock.0.norm.weight",
      "layer3.9.mblock.0.norm.bias",
      "layer3.9.mblock.1.branch_1.1.norm.weight",
      "layer3.9.mblock.1.branch_1.1.norm.bias",
      "layer3.9.mblock.1.branch_2.1.norm.weight",
      "layer3.9.mblock.1.branch_2.1.norm.bias",
      "layer3.9.mblock.1.branch_3.1.norm.weight",
      "layer3.9.mblock.1.branch_3.1.norm.bias",
      "layer3.9.mblock.2.norm.weight",
      "layer3.9.mblock.2.norm.bias",
      "layer3.10.sblock_in.norm.weight",
      "layer3.10.sblock_in.norm.bias",
      "layer3.10.sblock_dw.norm.weight",
      "layer3.10.sblock_dw.norm.bias",
      "layer3.10.sblock_proj.norm.weight",
      "layer3.10.sblock_proj.norm.bias",
      "layer3.10.mblock.0.norm.weight",
      "layer3.10.mblock.0.norm.bias",
      "layer3.10.mblock.1.branch_1.1.norm.weight",
      "layer3.10.mblock.1.branch_1.1.norm.bias",
      "layer3.10.mblock.1.branch_2.1.norm.weight",
      "layer3.10.mblock.1.branch_2.1.norm.bias",
      "layer3.10.mblock.1.branch_3.1.norm.weight",
      "layer3.10.mblock.1.branch_3.1.norm.bias",
      "layer3.10.mblock.2.norm.weight",
      "layer3.10.mblock.2.norm.bias",
      "layer3.11.sblock_in.norm.weight",
      "layer3.11.sblock_in.norm.bias",
      "layer3.11.sblock_dw.norm.weight",
      "layer3.11.sblock_dw.norm.bias",
      "layer3.11.sblock_proj.norm.weight",
      "layer3.11.sblock_proj.norm.bias",
      "layer3.11.mblock.0.norm.weight",
      "layer3.11.mblock.0.norm.bias",
      "layer3.11.mblock.1.branch_1.1.norm.weight",
      "layer3.11.mblock.1.branch_1.1.norm.bias",
      "layer3.11.mblock.1.branch_2.1.norm.weight",
      "layer3.11.mblock.1.branch_2.1.norm.bias",
      "layer3.11.mblock.1.branch_3.1.norm.weight",
      "layer3.11.mblock.1.branch_3.1.norm.bias",
      "layer3.11.mblock.2.norm.weight",
      "layer3.11.mblock.2.norm.bias",
      "layer3.12.sblock_in.norm.weight",
      "layer3.12.sblock_in.norm.bias",
      "layer3.12.sblock_dw.norm.weight",
      "layer3.12.sblock_dw.norm.bias",
      "layer3.12.sblock_proj.norm.weight",
      "layer3.12.sblock_proj.norm.bias",
      "layer3.12.mblock.0.norm.weight",
      "layer3.12.mblock.0.norm.bias",
      "layer3.12.mblock.1.branch_1.1.norm.weight",
      "layer3.12.mblock.1.branch_1.1.norm.bias",
      "layer3.12.mblock.1.branch_2.1.norm.weight",
      "layer3.12.mblock.1.branch_2.1.norm.bias",
      "layer3.12.mblock.1.branch_3.1.norm.weight",
      "layer3.12.mblock.1.branch_3.1.norm.bias",
      "layer3.12.mblock.2.norm.weight",
      "layer3.12.mblock.2.norm.bias",
      "layer3.13.sblock_in.norm.weight",
      "layer3.13.sblock_in.norm.bias",
      "layer3.13.sblock_dw.norm.weight",
      "layer3.13.sblock_dw.norm.bias",
      "layer3.13.sblock_proj.norm.weight",
      "layer3.13.sblock_proj.norm.bias",
      "layer3.13.mblock.0.norm.weight",
      "layer3.13.mblock.0.norm.bias",
      "layer3.13.mblock.1.branch_1.1.norm.weight",
      "layer3.13.mblock.1.branch_1.1.norm.bias",
      "layer3.13.mblock.1.branch_2.1.norm.weight",
      "layer3.13.mblock.1.branch_2.1.norm.bias",
      "layer3.13.mblock.1.branch_3.1.norm.weight",
      "layer3.13.mblock.1.branch_3.1.norm.bias",
      "layer3.13.mblock.2.norm.weight",
      "layer3.13.mblock.2.norm.bias",
      "layer3.14.sblock_in.norm.weight",
      "layer3.14.sblock_in.norm.bias",
      "layer3.14.sblock_dw.norm.weight",
      "layer3.14.sblock_dw.norm.bias",
      "layer3.14.sblock_proj.norm.weight",
      "layer3.14.sblock_proj.norm.bias",
      "layer3.14.mblock.0.norm.weight",
      "layer3.14.mblock.0.norm.bias",
      "layer3.14.mblock.1.branch_1.1.norm.weight",
      "layer3.14.mblock.1.branch_1.1.norm.bias",
      "layer3.14.mblock.1.branch_2.1.norm.weight",
      "layer3.14.mblock.1.branch_2.1.norm.bias",
      "layer3.14.mblock.1.branch_3.1.norm.weight",
      "layer3.14.mblock.1.branch_3.1.norm.bias",
      "layer3.14.mblock.2.norm.weight",
      "layer3.14.mblock.2.norm.bias",
      "layer3.15.sblock_in.norm.weight",
      "layer3.15.sblock_in.norm.bias",
      "layer3.15.sblock_dw.norm.weight",
      "layer3.15.sblock_dw.norm.bias",
      "layer3.15.sblock_proj.norm.weight",
      "layer3.15.sblock_proj.norm.bias",
      "layer3.15.mblock.0.norm.weight",
      "layer3.15.mblock.0.norm.bias",
      "layer3.15.mblock.1.branch_1.1.norm.weight",
      "layer3.15.mblock.1.branch_1.1.norm.bias",
      "layer3.15.mblock.1.branch_2.1.norm.weight",
      "layer3.15.mblock.1.branch_2.1.norm.bias",
      "layer3.15.mblock.1.branch_3.1.norm.weight",
      "layer3.15.mblock.1.branch_3.1.norm.bias",
      "layer3.15.mblock.2.norm.weight",
      "layer3.15.mblock.2.norm.bias",
      "layer3.16.sblock_in.norm.weight",
      "layer3.16.sblock_in.norm.bias",
      "layer3.16.sblock_dw.norm.weight",
      "layer3.16.sblock_dw.norm.bias",
      "layer3.16.sblock_proj.norm.weight",
      "layer3.16.sblock_proj.norm.bias",
      "layer3.16.mblock.0.norm.weight",
      "layer3.16.mblock.0.norm.bias",
      "layer3.16.mblock.1.branch_1.1.norm.weight",
      "layer3.16.mblock.1.branch_1.1.norm.bias",
      "layer3.16.mblock.1.branch_2.1.norm.weight",
      "layer3.16.mblock.1.branch_2.1.norm.bias",
      "layer3.16.mblock.1.branch_3.1.norm.weight",
      "layer3.16.mblock.1.branch_3.1.norm.bias",
      "layer3.16.mblock.2.norm.weight",
      "layer3.16.mblock.2.norm.bias",
      "layer4.0.mlp.0.norm.weight",
      "layer4.0.mlp.0.norm.bias",
      "layer4.0.mlp.1.norm.weight",
      "layer4.0.mlp.1.norm.bias",
      "layer4.0.mlp.2.norm.weight",
      "layer4.0.mlp.2.norm.bias",
      "layer4.0.skip.0.norm.weight",
      "layer4.0.skip.0.norm.bias",
      "layer4.0.skip.1.norm.weight",
      "layer4.0.skip.1.norm.bias",
      "layer4.1.sblock_in.norm.weight",
      "layer4.1.sblock_in.norm.bias",
      "layer4.1.sblock_dw.norm.weight",
      "layer4.1.sblock_dw.norm.bias",
      "layer4.1.sblock_proj.norm.weight",
      "layer4.1.sblock_proj.norm.bias",
      "layer4.1.mblock.0.norm.weight",
      "layer4.1.mblock.0.norm.bias",
      "layer4.1.mblock.1.branch_1.1.norm.weight",
      "layer4.1.mblock.1.branch_1.1.norm.bias",
      "layer4.1.mblock.1.branch_2.1.norm.weight",
      "layer4.1.mblock.1.branch_2.1.norm.bias",
      "layer4.1.mblock.1.branch_3.1.norm.weight",
      "layer4.1.mblock.1.branch_3.1.norm.bias",
      "layer4.1.mblock.2.norm.weight",
      "layer4.1.mblock.2.norm.bias",
      "layer4.2.sblock_in.norm.weight",
      "layer4.2.sblock_in.norm.bias",
      "layer4.2.sblock_dw.norm.weight",
      "layer4.2.sblock_dw.norm.bias",
      "layer4.2.sblock_proj.norm.weight",
      "layer4.2.sblock_proj.norm.bias",
      "layer4.2.mblock.0.norm.weight",
      "layer4.2.mblock.0.norm.bias",
      "layer4.2.mblock.1.branch_1.1.norm.weight",
      "layer4.2.mblock.1.branch_1.1.norm.bias",
      "layer4.2.mblock.1.branch_2.1.norm.weight",
      "layer4.2.mblock.1.branch_2.1.norm.bias",
      "layer4.2.mblock.1.branch_3.1.norm.weight",
      "layer4.2.mblock.1.branch_3.1.norm.bias",
      "layer4.2.mblock.2.norm.weight",
      "layer4.2.mblock.2.norm.bias",
      "layer4.3.sblock_in.norm.weight",
      "layer4.3.sblock_in.norm.bias",
      "layer4.3.sblock_dw.norm.weight",
      "layer4.3.sblock_dw.norm.bias",
      "layer4.3.sblock_proj.norm.weight",
      "layer4.3.sblock_proj.norm.bias",
      "layer4.3.mblock.0.norm.weight",
      "layer4.3.mblock.0.norm.bias",
      "layer4.3.mblock.1.branch_1.1.norm.weight",
      "layer4.3.mblock.1.branch_1.1.norm.bias",
      "layer4.3.mblock.1.branch_2.1.norm.weight",
      "layer4.3.mblock.1.branch_2.1.norm.bias",
      "layer4.3.mblock.1.branch_3.1.norm.weight",
      "layer4.3.mblock.1.branch_3.1.norm.bias",
      "layer4.3.mblock.2.norm.weight",
      "layer4.3.mblock.2.norm.bias",
      "layer4.4.sblock_in.norm.weight",
      "layer4.4.sblock_in.norm.bias",
      "layer4.4.sblock_dw.norm.weight",
      "layer4.4.sblock_dw.norm.bias",
      "layer4.4.sblock_proj.norm.weight",
      "layer4.4.sblock_proj.norm.bias",
      "layer4.4.mblock.0.norm.weight",
      "layer4.4.mblock.0.norm.bias",
      "layer4.4.mblock.1.branch_1.1.norm.weight",
      "layer4.4.mblock.1.branch_1.1.norm.bias",
      "layer4.4.mblock.1.branch_2.1.norm.weight",
      "layer4.4.mblock.1.branch_2.1.norm.bias",
      "layer4.4.mblock.1.branch_3.1.norm.weight",
      "layer4.4.mblock.1.branch_3.1.norm.bias",
      "layer4.4.mblock.2.norm.weight",
      "layer4.4.mblock.2.norm.bias",
      "head.norm.weight",
      "head.norm.bias",
      "classifier.norm.weight",
      "classifier.norm.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
Set warmup steps = 6255
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = LabelSmoothingCrossEntropy()
Auto resume checkpoint: 
Start training for 15 epochs
Epoch: [0]  [   0/5004]  eta: 13:20:27  lr: 0.000000  min_lr: 0.000000  loss: 2.6546 (2.6546)  class_acc: 0.6250 (0.6250)  weight_decay: 0.0500 (0.0500)  time: 9.5978  data: 2.3741  max mem: 31303
Epoch: [0]  [ 200/5004]  eta: 0:35:02  lr: 0.000000  min_lr: 0.000000  loss: 2.9372 (2.8446)  class_acc: 0.6250 (0.6273)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.5820 (nan)  time: 0.3939  data: 0.0002  max mem: 31303
Epoch: [0]  [ 400/5004]  eta: 0:31:45  lr: 0.000001  min_lr: 0.000001  loss: 2.9784 (2.8484)  class_acc: 0.5938 (0.6294)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.4175 (nan)  time: 0.3931  data: 0.0002  max mem: 31303
Epoch: [0]  [ 600/5004]  eta: 0:29:56  lr: 0.000001  min_lr: 0.000001  loss: 2.7903 (2.8439)  class_acc: 0.6250 (0.6291)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.2819 (nan)  time: 0.4040  data: 0.0002  max mem: 31303
Epoch: [0]  [ 800/5004]  eta: 0:28:22  lr: 0.000002  min_lr: 0.000002  loss: 2.7056 (2.8315)  class_acc: 0.6562 (0.6318)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3155 (nan)  time: 0.4054  data: 0.0002  max mem: 31303
Epoch: [0]  [1000/5004]  eta: 0:26:53  lr: 0.000002  min_lr: 0.000002  loss: 2.5074 (2.8139)  class_acc: 0.6875 (0.6324)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5688 (nan)  time: 0.3978  data: 0.0002  max mem: 31303
Epoch: [0]  [1200/5004]  eta: 0:25:25  lr: 0.000002  min_lr: 0.000002  loss: 2.5609 (2.7895)  class_acc: 0.6562 (0.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9360 (nan)  time: 0.3994  data: 0.0002  max mem: 31303
Epoch: [0]  [1400/5004]  eta: 0:24:01  lr: 0.000003  min_lr: 0.000003  loss: 2.5791 (2.7638)  class_acc: 0.6562 (0.6368)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1562 (nan)  time: 0.3916  data: 0.0002  max mem: 31303
Epoch: [0]  [1600/5004]  eta: 0:22:36  lr: 0.000003  min_lr: 0.000003  loss: 2.5581 (2.7386)  class_acc: 0.6250 (0.6397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6261 (nan)  time: 0.3847  data: 0.0002  max mem: 31303
Epoch: [0]  [1800/5004]  eta: 0:21:15  lr: 0.000004  min_lr: 0.000004  loss: 2.2448 (2.7082)  class_acc: 0.7188 (0.6434)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0972 (nan)  time: 0.3940  data: 0.0002  max mem: 31303
Epoch: [0]  [2000/5004]  eta: 0:19:54  lr: 0.000004  min_lr: 0.000004  loss: 2.3848 (2.6808)  class_acc: 0.6875 (0.6471)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7066 (nan)  time: 0.3872  data: 0.0002  max mem: 31303
Epoch: [0]  [2200/5004]  eta: 0:18:33  lr: 0.000004  min_lr: 0.000004  loss: 2.2524 (2.6548)  class_acc: 0.7188 (0.6506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7381 (nan)  time: 0.3862  data: 0.0002  max mem: 31303
Epoch: [0]  [2400/5004]  eta: 0:17:12  lr: 0.000005  min_lr: 0.000005  loss: 2.4123 (2.6319)  class_acc: 0.6875 (0.6532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7948 (nan)  time: 0.3898  data: 0.0002  max mem: 31303
Epoch: [0]  [2600/5004]  eta: 0:15:52  lr: 0.000005  min_lr: 0.000005  loss: 2.3852 (2.6102)  class_acc: 0.6250 (0.6560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8656 (nan)  time: 0.3896  data: 0.0002  max mem: 31303
Epoch: [0]  [2800/5004]  eta: 0:14:33  lr: 0.000006  min_lr: 0.000006  loss: 2.1642 (2.5896)  class_acc: 0.6875 (0.6589)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1940 (nan)  time: 0.3959  data: 0.0002  max mem: 31303
Epoch: [0]  [3000/5004]  eta: 0:13:14  lr: 0.000006  min_lr: 0.000006  loss: 2.2521 (2.5714)  class_acc: 0.6875 (0.6613)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2472 (nan)  time: 0.3963  data: 0.0002  max mem: 31303
Epoch: [0]  [3200/5004]  eta: 0:11:55  lr: 0.000006  min_lr: 0.000006  loss: 2.1687 (2.5526)  class_acc: 0.7188 (0.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9723 (nan)  time: 0.3912  data: 0.0002  max mem: 31303
Epoch: [0]  [3400/5004]  eta: 0:10:35  lr: 0.000007  min_lr: 0.000007  loss: 2.1985 (2.5352)  class_acc: 0.7188 (0.6656)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1220 (nan)  time: 0.4038  data: 0.0002  max mem: 31303
Epoch: [0]  [3600/5004]  eta: 0:09:16  lr: 0.000007  min_lr: 0.000007  loss: 2.1780 (2.5165)  class_acc: 0.6875 (0.6682)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6732 (nan)  time: 0.3958  data: 0.0002  max mem: 31303
Epoch: [0]  [3800/5004]  eta: 0:07:56  lr: 0.000008  min_lr: 0.000008  loss: 2.1590 (2.4985)  class_acc: 0.7188 (0.6710)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5339 (nan)  time: 0.3945  data: 0.0002  max mem: 31303
Epoch: [0]  [4000/5004]  eta: 0:06:37  lr: 0.000008  min_lr: 0.000008  loss: 2.1339 (2.4844)  class_acc: 0.7188 (0.6731)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4725 (nan)  time: 0.3866  data: 0.0002  max mem: 31303
Epoch: [0]  [4200/5004]  eta: 0:05:18  lr: 0.000008  min_lr: 0.000008  loss: 2.1731 (2.4704)  class_acc: 0.7188 (0.6752)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1734 (nan)  time: 0.3857  data: 0.0002  max mem: 31303
Epoch: [0]  [4400/5004]  eta: 0:03:58  lr: 0.000009  min_lr: 0.000009  loss: 2.2487 (2.4562)  class_acc: 0.7188 (0.6771)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8829 (nan)  time: 0.4022  data: 0.0002  max mem: 31303
Epoch: [0]  [4600/5004]  eta: 0:02:39  lr: 0.000009  min_lr: 0.000009  loss: 2.0975 (2.4438)  class_acc: 0.7500 (0.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6638 (nan)  time: 0.3977  data: 0.0002  max mem: 31303
Epoch: [0]  [4800/5004]  eta: 0:01:20  lr: 0.000010  min_lr: 0.000010  loss: 2.2318 (2.4330)  class_acc: 0.6875 (0.6805)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0725 (nan)  time: 0.3918  data: 0.0002  max mem: 31303
Epoch: [0]  [5000/5004]  eta: 0:00:01  lr: 0.000010  min_lr: 0.000010  loss: 1.9901 (2.4212)  class_acc: 0.7500 (0.6822)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7853 (nan)  time: 0.3844  data: 0.0005  max mem: 31303
Epoch: [0]  [5003/5004]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000010  loss: 1.9866 (2.4209)  class_acc: 0.7500 (0.6822)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7853 (nan)  time: 0.3831  data: 0.0005  max mem: 31303
Epoch: [0] Total time: 0:32:59 (0.3955 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 1.9866 (2.4173)  class_acc: 0.7500 (0.6833)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7853 (nan)
Test:  [ 0/25]  eta: 0:08:42  loss: 0.3988 (0.3988)  acc1: 90.8000 (90.8000)  acc5: 99.6000 (99.6000)  time: 20.9179  data: 12.2362  max mem: 31303
Test:  [10/25]  eta: 0:00:37  loss: 0.5409 (0.5335)  acc1: 87.6000 (87.9636)  acc5: 97.6000 (97.9636)  time: 2.5331  data: 1.1126  max mem: 31303
Test:  [20/25]  eta: 0:00:08  loss: 0.6124 (0.6130)  acc1: 84.0000 (85.2000)  acc5: 96.8000 (97.3333)  time: 0.6949  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6442 (0.6270)  acc1: 83.6000 (84.7520)  acc5: 96.8000 (97.2320)  time: 0.6950  data: 0.0001  max mem: 31303
Test: Total time: 0:00:37 (1.5087 s / it)
* Acc@1 84.894 Acc@5 97.334 loss 0.615
Accuracy of the model on the 50000 test images: 84.9%
Max accuracy: 84.89%
Test:  [ 0/25]  eta: 0:03:54  loss: 0.9815 (0.9815)  acc1: 92.0000 (92.0000)  acc5: 99.6000 (99.6000)  time: 9.3781  data: 8.6354  max mem: 31303
Test:  [10/25]  eta: 0:00:22  loss: 1.1396 (1.1361)  acc1: 87.6000 (87.6727)  acc5: 97.6000 (98.0000)  time: 1.5002  data: 0.7853  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 1.1605 (1.1638)  acc1: 84.0000 (85.0095)  acc5: 97.6000 (97.4857)  time: 0.7042  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 1.1605 (1.1587)  acc1: 84.0000 (84.6560)  acc5: 97.2000 (97.4080)  time: 0.6997  data: 0.0001  max mem: 31303
Test: Total time: 0:00:26 (1.0600 s / it)
* Acc@1 84.938 Acc@5 97.278 loss 1.143
Accuracy of the model EMA on 50000 test images: 84.9%
Max EMA accuracy: 84.94%
Epoch: [1]  [   0/5004]  eta: 1:45:57  lr: 0.000010  min_lr: 0.000010  loss: 1.6936 (1.6936)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 1.2705  data: 0.8225  max mem: 31303
Epoch: [1]  [ 200/5004]  eta: 0:31:39  lr: 0.000010  min_lr: 0.000010  loss: 2.1453 (2.1175)  class_acc: 0.7500 (0.7293)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1745 (5.4000)  time: 0.3844  data: 0.0002  max mem: 31303
Epoch: [1]  [ 400/5004]  eta: 0:30:11  lr: 0.000011  min_lr: 0.000011  loss: 2.1952 (2.1362)  class_acc: 0.7188 (0.7258)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8124 (5.3689)  time: 0.3901  data: 0.0002  max mem: 31303
Epoch: [1]  [ 600/5004]  eta: 0:28:43  lr: 0.000011  min_lr: 0.000011  loss: 2.0654 (2.1209)  class_acc: 0.7188 (0.7295)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8770 (5.4197)  time: 0.3858  data: 0.0002  max mem: 31303
Epoch: [1]  [ 800/5004]  eta: 0:27:25  lr: 0.000012  min_lr: 0.000012  loss: 2.1041 (2.1346)  class_acc: 0.7500 (0.7246)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8683 (5.3976)  time: 0.3856  data: 0.0002  max mem: 31303
Epoch: [1]  [1000/5004]  eta: 0:26:02  lr: 0.000012  min_lr: 0.000012  loss: 1.9206 (2.1222)  class_acc: 0.7500 (0.7268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0675 (5.4040)  time: 0.3820  data: 0.0002  max mem: 31303
Epoch: [1]  [1200/5004]  eta: 0:24:41  lr: 0.000012  min_lr: 0.000012  loss: 2.1379 (2.1220)  class_acc: 0.7188 (0.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7223 (5.5030)  time: 0.3932  data: 0.0002  max mem: 31303
Epoch: [1]  [1400/5004]  eta: 0:23:23  lr: 0.000013  min_lr: 0.000013  loss: 2.0619 (2.1187)  class_acc: 0.7500 (0.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9543 (5.5714)  time: 0.3936  data: 0.0002  max mem: 31303
Epoch: [1]  [1600/5004]  eta: 0:22:04  lr: 0.000013  min_lr: 0.000013  loss: 2.1569 (2.1243)  class_acc: 0.7500 (0.7270)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7234 (5.5784)  time: 0.3997  data: 0.0002  max mem: 31303
Epoch: [1]  [1800/5004]  eta: 0:20:46  lr: 0.000014  min_lr: 0.000014  loss: 2.0988 (2.1196)  class_acc: 0.7500 (0.7280)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2891 (5.5674)  time: 0.3853  data: 0.0002  max mem: 31303
Epoch: [1]  [2000/5004]  eta: 0:19:27  lr: 0.000014  min_lr: 0.000014  loss: 2.1438 (2.1144)  class_acc: 0.7188 (0.7294)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6193 (5.4781)  time: 0.3836  data: 0.0002  max mem: 31303
Epoch: [1]  [2200/5004]  eta: 0:18:08  lr: 0.000014  min_lr: 0.000014  loss: 2.1128 (2.1136)  class_acc: 0.7188 (0.7294)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0944 (5.5191)  time: 0.3834  data: 0.0002  max mem: 31303
Epoch: [1]  [2400/5004]  eta: 0:16:50  lr: 0.000015  min_lr: 0.000015  loss: 1.9675 (2.1110)  class_acc: 0.7500 (0.7294)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2265 (5.5902)  time: 0.3836  data: 0.0002  max mem: 31303
Epoch: [1]  [2600/5004]  eta: 0:15:33  lr: 0.000015  min_lr: 0.000015  loss: 2.0517 (2.1102)  class_acc: 0.7188 (0.7291)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8735 (5.6324)  time: 0.3848  data: 0.0002  max mem: 31303
Epoch: [1]  [2800/5004]  eta: 0:14:15  lr: 0.000016  min_lr: 0.000016  loss: 1.9029 (2.1086)  class_acc: 0.7500 (0.7297)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0221 (5.5837)  time: 0.3904  data: 0.0002  max mem: 31303
Epoch: [1]  [3000/5004]  eta: 0:12:58  lr: 0.000016  min_lr: 0.000016  loss: 2.0683 (2.1084)  class_acc: 0.6875 (0.7293)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3865 (5.5673)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [1]  [3200/5004]  eta: 0:11:40  lr: 0.000016  min_lr: 0.000016  loss: 2.0549 (2.1064)  class_acc: 0.7500 (0.7294)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7826 (5.5155)  time: 0.3840  data: 0.0002  max mem: 31303
Epoch: [1]  [3400/5004]  eta: 0:10:22  lr: 0.000017  min_lr: 0.000017  loss: 1.9608 (2.1052)  class_acc: 0.7812 (0.7297)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8517 (5.5192)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [1]  [3600/5004]  eta: 0:09:04  lr: 0.000017  min_lr: 0.000017  loss: 2.0374 (2.1033)  class_acc: 0.7500 (0.7298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6104 (5.5173)  time: 0.3863  data: 0.0002  max mem: 31303
Epoch: [1]  [3800/5004]  eta: 0:07:47  lr: 0.000018  min_lr: 0.000018  loss: 2.1826 (2.1029)  class_acc: 0.6562 (0.7298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2730 (5.5120)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [1]  [4000/5004]  eta: 0:06:29  lr: 0.000018  min_lr: 0.000018  loss: 1.9758 (2.1002)  class_acc: 0.7500 (0.7302)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9710 (5.5409)  time: 0.3833  data: 0.0002  max mem: 31303
Epoch: [1]  [4200/5004]  eta: 0:05:11  lr: 0.000018  min_lr: 0.000018  loss: 2.0921 (2.0989)  class_acc: 0.7500 (0.7306)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4499 (5.5257)  time: 0.3844  data: 0.0002  max mem: 31303
Epoch: [1]  [4400/5004]  eta: 0:03:54  lr: 0.000019  min_lr: 0.000019  loss: 2.0188 (2.0966)  class_acc: 0.7500 (0.7310)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0215 (5.5218)  time: 0.3894  data: 0.0002  max mem: 31303
Epoch: [1]  [4600/5004]  eta: 0:02:36  lr: 0.000019  min_lr: 0.000019  loss: 1.9449 (2.0957)  class_acc: 0.7812 (0.7311)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9762 (5.5139)  time: 0.3842  data: 0.0002  max mem: 31303
Epoch: [1]  [4800/5004]  eta: 0:01:19  lr: 0.000020  min_lr: 0.000020  loss: 1.9870 (2.0936)  class_acc: 0.7812 (0.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0317 (5.5096)  time: 0.3851  data: 0.0002  max mem: 31303
Epoch: [1]  [5000/5004]  eta: 0:00:01  lr: 0.000020  min_lr: 0.000020  loss: 1.9922 (2.0929)  class_acc: 0.7188 (0.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0614 (5.4936)  time: 0.3859  data: 0.0005  max mem: 31303
Epoch: [1]  [5003/5004]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000020  loss: 2.0256 (2.0930)  class_acc: 0.7188 (0.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0614 (5.4919)  time: 0.3852  data: 0.0005  max mem: 31303
Epoch: [1] Total time: 0:32:21 (0.3881 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000020  loss: 2.0256 (2.0943)  class_acc: 0.7188 (0.7300)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0614 (5.4919)
Test:  [ 0/25]  eta: 0:04:51  loss: 0.4067 (0.4067)  acc1: 90.8000 (90.8000)  acc5: 99.2000 (99.2000)  time: 11.6774  data: 10.9593  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5496 (0.5293)  acc1: 87.2000 (87.8909)  acc5: 98.0000 (98.0727)  time: 1.6994  data: 0.9966  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6204 (0.6084)  acc1: 85.2000 (85.2381)  acc5: 97.2000 (97.4476)  time: 0.6979  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6707 (0.6225)  acc1: 84.8000 (84.7360)  acc5: 97.2000 (97.3600)  time: 0.6953  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1437 s / it)
* Acc@1 85.108 Acc@5 97.404 loss 0.610
Accuracy of the model on the 50000 test images: 85.1%
Max accuracy: 85.11%
Test:  [ 0/25]  eta: 0:05:00  loss: 0.8003 (0.8003)  acc1: 92.0000 (92.0000)  acc5: 99.2000 (99.2000)  time: 12.0018  data: 11.2781  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.9489 (0.9488)  acc1: 88.0000 (87.7818)  acc5: 97.6000 (97.9636)  time: 1.7216  data: 1.0255  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.9931 (0.9914)  acc1: 84.0000 (85.1238)  acc5: 97.2000 (97.4667)  time: 0.6938  data: 0.0001  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.9988 (0.9911)  acc1: 84.0000 (84.7040)  acc5: 97.2000 (97.3440)  time: 0.6939  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1542 s / it)
* Acc@1 85.162 Acc@5 97.354 loss 0.976
Accuracy of the model EMA on 50000 test images: 85.2%
Max EMA accuracy: 85.16%
Epoch: [2]  [   0/5004]  eta: 1:47:22  lr: 0.000020  min_lr: 0.000020  loss: 1.8081 (1.8081)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 1.2874  data: 0.9123  max mem: 31303
Epoch: [2]  [ 200/5004]  eta: 0:31:10  lr: 0.000020  min_lr: 0.000020  loss: 2.1731 (2.0283)  class_acc: 0.7188 (0.7436)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2455 (5.5806)  time: 0.3935  data: 0.0002  max mem: 31303
Epoch: [2]  [ 400/5004]  eta: 0:29:46  lr: 0.000021  min_lr: 0.000021  loss: 1.9423 (2.0399)  class_acc: 0.7188 (0.7400)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1786 (5.2696)  time: 0.4001  data: 0.0002  max mem: 31303
Epoch: [2]  [ 600/5004]  eta: 0:28:29  lr: 0.000021  min_lr: 0.000021  loss: 1.9861 (2.0437)  class_acc: 0.7188 (0.7395)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8492 (5.3088)  time: 0.3855  data: 0.0002  max mem: 31303
Epoch: [2]  [ 800/5004]  eta: 0:27:12  lr: 0.000022  min_lr: 0.000022  loss: 1.9813 (2.0440)  class_acc: 0.7500 (0.7383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1877 (5.3657)  time: 0.3858  data: 0.0002  max mem: 31303
Epoch: [2]  [1000/5004]  eta: 0:25:54  lr: 0.000022  min_lr: 0.000022  loss: 1.9808 (2.0421)  class_acc: 0.7188 (0.7371)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8090 (5.4753)  time: 0.3841  data: 0.0002  max mem: 31303
Epoch: [2]  [1200/5004]  eta: 0:24:37  lr: 0.000022  min_lr: 0.000022  loss: 2.0394 (2.0426)  class_acc: 0.7500 (0.7374)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4636 (5.4355)  time: 0.3862  data: 0.0002  max mem: 31303
Epoch: [2]  [1400/5004]  eta: 0:23:19  lr: 0.000023  min_lr: 0.000023  loss: 2.0809 (2.0440)  class_acc: 0.7188 (0.7368)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3481 (5.3885)  time: 0.3826  data: 0.0002  max mem: 31303
Epoch: [2]  [1600/5004]  eta: 0:22:00  lr: 0.000023  min_lr: 0.000023  loss: 2.0703 (2.0420)  class_acc: 0.7188 (0.7372)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5461 (5.3581)  time: 0.3844  data: 0.0002  max mem: 31303
Epoch: [2]  [1800/5004]  eta: 0:20:42  lr: 0.000024  min_lr: 0.000024  loss: 2.0882 (2.0444)  class_acc: 0.6562 (0.7358)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6727 (5.3085)  time: 0.3867  data: 0.0002  max mem: 31303
Epoch: [2]  [2000/5004]  eta: 0:19:27  lr: 0.000024  min_lr: 0.000024  loss: 1.9696 (2.0469)  class_acc: 0.7500 (0.7354)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2885 (5.3536)  time: 0.3935  data: 0.0002  max mem: 31303
Epoch: [2]  [2200/5004]  eta: 0:18:10  lr: 0.000024  min_lr: 0.000024  loss: 2.1103 (2.0446)  class_acc: 0.7500 (0.7361)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2181 (5.3008)  time: 0.4012  data: 0.0002  max mem: 31303
Epoch: [2]  [2400/5004]  eta: 0:16:53  lr: 0.000025  min_lr: 0.000025  loss: 1.9770 (2.0423)  class_acc: 0.7188 (0.7371)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1575 (5.2616)  time: 0.4011  data: 0.0002  max mem: 31303
Epoch: [2]  [2600/5004]  eta: 0:15:34  lr: 0.000025  min_lr: 0.000025  loss: 2.0073 (2.0391)  class_acc: 0.7500 (0.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2365 (5.2837)  time: 0.3945  data: 0.0003  max mem: 31303
Epoch: [2]  [2800/5004]  eta: 0:14:16  lr: 0.000026  min_lr: 0.000026  loss: 1.8741 (2.0371)  class_acc: 0.7500 (0.7386)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9218 (5.3041)  time: 0.3897  data: 0.0002  max mem: 31303
Epoch: [2]  [3000/5004]  eta: 0:12:58  lr: 0.000026  min_lr: 0.000026  loss: 2.0662 (2.0372)  class_acc: 0.7188 (0.7384)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7184 (5.3472)  time: 0.3839  data: 0.0002  max mem: 31303
Epoch: [2]  [3200/5004]  eta: 0:11:41  lr: 0.000026  min_lr: 0.000026  loss: 1.9810 (2.0358)  class_acc: 0.7500 (0.7389)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5662 (5.3765)  time: 0.3879  data: 0.0002  max mem: 31303
Epoch: [2]  [3400/5004]  eta: 0:10:23  lr: 0.000027  min_lr: 0.000027  loss: 2.0465 (2.0360)  class_acc: 0.7500 (0.7393)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9073 (5.3803)  time: 0.3932  data: 0.0002  max mem: 31303
Epoch: [2]  [3600/5004]  eta: 0:09:06  lr: 0.000027  min_lr: 0.000027  loss: 1.9445 (2.0359)  class_acc: 0.7500 (0.7397)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4043 (5.3693)  time: 0.3871  data: 0.0002  max mem: 31303
Epoch: [2]  [3800/5004]  eta: 0:07:48  lr: 0.000028  min_lr: 0.000028  loss: 2.0120 (2.0364)  class_acc: 0.7188 (0.7392)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3561 (5.3430)  time: 0.3868  data: 0.0002  max mem: 31303
Epoch: [2]  [4000/5004]  eta: 0:06:30  lr: 0.000028  min_lr: 0.000028  loss: 1.9147 (2.0346)  class_acc: 0.7500 (0.7395)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7836 (5.3179)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [2]  [4200/5004]  eta: 0:05:12  lr: 0.000028  min_lr: 0.000028  loss: 1.9691 (2.0347)  class_acc: 0.7500 (0.7391)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8406 (5.3239)  time: 0.3889  data: 0.0002  max mem: 31303
Epoch: [2]  [4400/5004]  eta: 0:03:54  lr: 0.000029  min_lr: 0.000029  loss: 2.0109 (2.0347)  class_acc: 0.7188 (0.7392)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9981 (5.3851)  time: 0.3890  data: 0.0002  max mem: 31303
Epoch: [2]  [4600/5004]  eta: 0:02:37  lr: 0.000029  min_lr: 0.000029  loss: 1.9839 (2.0349)  class_acc: 0.7188 (0.7392)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5333 (5.4110)  time: 0.3866  data: 0.0002  max mem: 31303
Epoch: [2]  [4800/5004]  eta: 0:01:19  lr: 0.000030  min_lr: 0.000030  loss: 2.0903 (2.0327)  class_acc: 0.7500 (0.7397)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5613 (5.3948)  time: 0.3854  data: 0.0002  max mem: 31303
Epoch: [2]  [5000/5004]  eta: 0:00:01  lr: 0.000030  min_lr: 0.000030  loss: 2.0628 (2.0324)  class_acc: 0.7188 (0.7399)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3586 (5.3789)  time: 0.3884  data: 0.0006  max mem: 31303
Epoch: [2]  [5003/5004]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000030  loss: 2.0628 (2.0324)  class_acc: 0.7188 (0.7399)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8495 (5.3793)  time: 0.3873  data: 0.0006  max mem: 31303
Epoch: [2] Total time: 0:32:28 (0.3893 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000030  loss: 2.0628 (2.0297)  class_acc: 0.7188 (0.7413)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8495 (5.3793)
Test:  [ 0/25]  eta: 0:04:45  loss: 0.4034 (0.4034)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 11.4232  data: 10.7033  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5635 (0.5371)  acc1: 86.8000 (87.7091)  acc5: 98.0000 (98.1091)  time: 1.6745  data: 0.9733  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6257 (0.6181)  acc1: 84.0000 (85.2000)  acc5: 97.2000 (97.4857)  time: 0.6972  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6814 (0.6316)  acc1: 84.0000 (84.7360)  acc5: 96.8000 (97.3600)  time: 0.6951  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1335 s / it)
* Acc@1 85.086 Acc@5 97.338 loss 0.619
Accuracy of the model on the 50000 test images: 85.1%
Max accuracy: 85.11%
Test:  [ 0/25]  eta: 0:04:44  loss: 0.6804 (0.6804)  acc1: 92.4000 (92.4000)  acc5: 99.6000 (99.6000)  time: 11.3750  data: 10.6405  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.8273 (0.8272)  acc1: 87.6000 (87.8546)  acc5: 98.0000 (98.0727)  time: 1.6668  data: 0.9676  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.8852 (0.8814)  acc1: 84.4000 (85.1429)  acc5: 97.6000 (97.5048)  time: 0.6951  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.9056 (0.8847)  acc1: 83.6000 (84.6880)  acc5: 97.2000 (97.4240)  time: 0.6946  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1292 s / it)
* Acc@1 85.236 Acc@5 97.406 loss 0.870
Accuracy of the model EMA on 50000 test images: 85.2%
Max EMA accuracy: 85.24%
Epoch: [3]  [   0/5004]  eta: 1:50:58  lr: 0.000030  min_lr: 0.000030  loss: 1.6827 (1.6827)  class_acc: 0.8125 (0.8125)  weight_decay: 0.0500 (0.0500)  time: 1.3306  data: 0.9517  max mem: 31303
Epoch: [3]  [ 200/5004]  eta: 0:31:28  lr: 0.000030  min_lr: 0.000030  loss: 2.0598 (2.0285)  class_acc: 0.7188 (0.7424)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2762 (5.0163)  time: 0.3857  data: 0.0002  max mem: 31303
Epoch: [3]  [ 400/5004]  eta: 0:30:04  lr: 0.000031  min_lr: 0.000031  loss: 1.9731 (2.0013)  class_acc: 0.7500 (0.7471)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6600 (5.0558)  time: 0.3964  data: 0.0002  max mem: 31303
Epoch: [3]  [ 600/5004]  eta: 0:28:37  lr: 0.000031  min_lr: 0.000031  loss: 2.0027 (2.0065)  class_acc: 0.7500 (0.7468)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5655 (5.1505)  time: 0.3870  data: 0.0002  max mem: 31303
Epoch: [3]  [ 800/5004]  eta: 0:27:21  lr: 0.000032  min_lr: 0.000032  loss: 2.0916 (2.0140)  class_acc: 0.7188 (0.7433)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7553 (5.3910)  time: 0.3924  data: 0.0002  max mem: 31303
Epoch: [3]  [1000/5004]  eta: 0:26:00  lr: 0.000032  min_lr: 0.000032  loss: 1.9944 (2.0145)  class_acc: 0.7188 (0.7446)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8019 (5.4439)  time: 0.3861  data: 0.0002  max mem: 31303
Epoch: [3]  [1200/5004]  eta: 0:24:41  lr: 0.000032  min_lr: 0.000032  loss: 1.9486 (2.0137)  class_acc: 0.7812 (0.7451)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5097 (5.5678)  time: 0.3832  data: 0.0002  max mem: 31303
Epoch: [3]  [1400/5004]  eta: 0:23:23  lr: 0.000033  min_lr: 0.000033  loss: 1.9728 (2.0120)  class_acc: 0.7500 (0.7443)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6786 (5.5481)  time: 0.3918  data: 0.0002  max mem: 31303
Epoch: [3]  [1600/5004]  eta: 0:22:07  lr: 0.000033  min_lr: 0.000033  loss: 1.8636 (2.0086)  class_acc: 0.7500 (0.7455)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8012 (5.5668)  time: 0.3973  data: 0.0002  max mem: 31303
Epoch: [3]  [1800/5004]  eta: 0:20:50  lr: 0.000034  min_lr: 0.000034  loss: 2.0049 (2.0102)  class_acc: 0.7188 (0.7457)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5252 (5.5581)  time: 0.3901  data: 0.0002  max mem: 31303
Epoch: [3]  [2000/5004]  eta: 0:19:32  lr: 0.000034  min_lr: 0.000034  loss: 2.0485 (2.0117)  class_acc: 0.7500 (0.7450)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8296 (5.5572)  time: 0.3890  data: 0.0002  max mem: 31303
Epoch: [3]  [2200/5004]  eta: 0:18:13  lr: 0.000034  min_lr: 0.000034  loss: 1.9166 (2.0116)  class_acc: 0.7188 (0.7446)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9839 (5.5639)  time: 0.3857  data: 0.0002  max mem: 31303
Epoch: [3]  [2400/5004]  eta: 0:16:54  lr: 0.000035  min_lr: 0.000035  loss: 1.9679 (2.0099)  class_acc: 0.7500 (0.7451)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9301 (5.5444)  time: 0.3827  data: 0.0002  max mem: 31303
Epoch: [3]  [2600/5004]  eta: 0:15:35  lr: 0.000035  min_lr: 0.000035  loss: 1.9988 (2.0107)  class_acc: 0.7188 (0.7448)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7337 (5.4773)  time: 0.3955  data: 0.0002  max mem: 31303
Epoch: [3]  [2800/5004]  eta: 0:14:17  lr: 0.000036  min_lr: 0.000036  loss: 1.9128 (2.0112)  class_acc: 0.7812 (0.7448)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3134 (5.4626)  time: 0.3832  data: 0.0002  max mem: 31303
Epoch: [3]  [3000/5004]  eta: 0:12:59  lr: 0.000036  min_lr: 0.000036  loss: 1.9577 (2.0103)  class_acc: 0.7500 (0.7449)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5179 (5.4332)  time: 0.3863  data: 0.0002  max mem: 31303
Epoch: [3]  [3200/5004]  eta: 0:11:41  lr: 0.000036  min_lr: 0.000036  loss: 2.0727 (2.0086)  class_acc: 0.7188 (0.7455)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4988 (5.4383)  time: 0.3848  data: 0.0002  max mem: 31303
Epoch: [3]  [3400/5004]  eta: 0:10:23  lr: 0.000037  min_lr: 0.000037  loss: 1.9151 (2.0071)  class_acc: 0.7188 (0.7460)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5610 (5.4234)  time: 0.3828  data: 0.0002  max mem: 31303
Epoch: [3]  [3600/5004]  eta: 0:09:05  lr: 0.000037  min_lr: 0.000037  loss: 1.9418 (2.0052)  class_acc: 0.7812 (0.7463)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9801 (5.4340)  time: 0.3958  data: 0.0002  max mem: 31303
Epoch: [3]  [3800/5004]  eta: 0:07:47  lr: 0.000038  min_lr: 0.000038  loss: 1.9961 (2.0039)  class_acc: 0.7500 (0.7467)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9616 (5.4137)  time: 0.3854  data: 0.0002  max mem: 31303
Epoch: [3]  [4000/5004]  eta: 0:06:29  lr: 0.000038  min_lr: 0.000038  loss: 1.9417 (2.0034)  class_acc: 0.7500 (0.7466)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0261 (5.4004)  time: 0.3974  data: 0.0002  max mem: 31303
Epoch: [3]  [4200/5004]  eta: 0:05:12  lr: 0.000038  min_lr: 0.000038  loss: 1.9538 (2.0028)  class_acc: 0.7188 (0.7471)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0166 (5.3892)  time: 0.3786  data: 0.0002  max mem: 31303
Epoch: [3]  [4400/5004]  eta: 0:03:54  lr: 0.000039  min_lr: 0.000039  loss: 2.0344 (2.0022)  class_acc: 0.7188 (0.7471)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6359 (5.3914)  time: 0.3869  data: 0.0002  max mem: 31303
Epoch: [3]  [4600/5004]  eta: 0:02:36  lr: 0.000039  min_lr: 0.000039  loss: 1.9672 (2.0022)  class_acc: 0.7500 (0.7468)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8612 (5.3728)  time: 0.3940  data: 0.0002  max mem: 31303
Epoch: [3]  [4800/5004]  eta: 0:01:19  lr: 0.000040  min_lr: 0.000040  loss: 2.0245 (2.0017)  class_acc: 0.7500 (0.7467)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3749 (5.3921)  time: 0.3812  data: 0.0002  max mem: 31303
Epoch: [3]  [5000/5004]  eta: 0:00:01  lr: 0.000040  min_lr: 0.000040  loss: 1.9230 (2.0028)  class_acc: 0.7500 (0.7464)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9622 (5.3935)  time: 0.3763  data: 0.0005  max mem: 31303
Epoch: [3]  [5003/5004]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000040  loss: 1.9330 (2.0028)  class_acc: 0.7812 (0.7464)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9622 (5.3916)  time: 0.3756  data: 0.0005  max mem: 31303
Epoch: [3] Total time: 0:32:20 (0.3878 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000040  loss: 1.9330 (1.9965)  class_acc: 0.7812 (0.7477)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9622 (5.3916)
Test:  [ 0/25]  eta: 0:04:15  loss: 0.3983 (0.3983)  acc1: 91.2000 (91.2000)  acc5: 99.2000 (99.2000)  time: 10.2053  data: 9.4918  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5511 (0.5384)  acc1: 87.6000 (87.7818)  acc5: 97.6000 (98.0000)  time: 1.5702  data: 0.8632  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6359 (0.6187)  acc1: 84.4000 (85.1429)  acc5: 96.8000 (97.3905)  time: 0.7006  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6824 (0.6328)  acc1: 84.0000 (84.6560)  acc5: 96.8000 (97.2960)  time: 0.6969  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.0870 s / it)
* Acc@1 85.144 Acc@5 97.362 loss 0.622
Accuracy of the model on the 50000 test images: 85.1%
Max accuracy: 85.14%
Test:  [ 0/25]  eta: 0:04:40  loss: 0.5956 (0.5956)  acc1: 92.4000 (92.4000)  acc5: 99.6000 (99.6000)  time: 11.2263  data: 10.5222  max mem: 31303
Test:  [10/25]  eta: 0:00:24  loss: 0.7419 (0.7418)  acc1: 87.2000 (87.6727)  acc5: 98.0000 (98.1091)  time: 1.6516  data: 0.9568  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.8162 (0.8045)  acc1: 84.0000 (85.2191)  acc5: 97.2000 (97.5238)  time: 0.6939  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.8408 (0.8105)  acc1: 83.6000 (84.7680)  acc5: 97.2000 (97.4560)  time: 0.6938  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1220 s / it)
* Acc@1 85.290 Acc@5 97.464 loss 0.796
Accuracy of the model EMA on 50000 test images: 85.3%
Max EMA accuracy: 85.29%
Epoch: [4]  [   0/5004]  eta: 1:47:14  lr: 0.000040  min_lr: 0.000040  loss: 1.9429 (1.9429)  class_acc: 0.6562 (0.6562)  weight_decay: 0.0500 (0.0500)  time: 1.2860  data: 0.9077  max mem: 31303
Epoch: [4]  [ 200/5004]  eta: 0:31:33  lr: 0.000040  min_lr: 0.000040  loss: 1.8677 (1.9743)  class_acc: 0.7812 (0.7522)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7755 (6.2851)  time: 0.3847  data: 0.0002  max mem: 31303
Epoch: [4]  [ 400/5004]  eta: 0:30:00  lr: 0.000041  min_lr: 0.000041  loss: 2.0795 (1.9761)  class_acc: 0.7188 (0.7512)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4599 (6.0428)  time: 0.3818  data: 0.0002  max mem: 31303
Epoch: [4]  [ 600/5004]  eta: 0:28:34  lr: 0.000041  min_lr: 0.000041  loss: 1.9694 (1.9860)  class_acc: 0.7188 (0.7501)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0232 (5.7298)  time: 0.3813  data: 0.0002  max mem: 31303
Epoch: [4]  [ 800/5004]  eta: 0:27:10  lr: 0.000042  min_lr: 0.000042  loss: 2.2144 (1.9891)  class_acc: 0.6875 (0.7498)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9074 (5.6455)  time: 0.3799  data: 0.0002  max mem: 31303
Epoch: [4]  [1000/5004]  eta: 0:25:50  lr: 0.000042  min_lr: 0.000042  loss: 1.8592 (1.9804)  class_acc: 0.8125 (0.7527)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9412 (5.6309)  time: 0.3877  data: 0.0002  max mem: 31303
Epoch: [4]  [1200/5004]  eta: 0:24:34  lr: 0.000042  min_lr: 0.000042  loss: 1.9702 (1.9817)  class_acc: 0.7188 (0.7512)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3314 (5.6807)  time: 0.3954  data: 0.0002  max mem: 31303
Epoch: [4]  [1400/5004]  eta: 0:23:19  lr: 0.000043  min_lr: 0.000043  loss: 1.9276 (1.9757)  class_acc: 0.7500 (0.7530)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8589 (5.6283)  time: 0.3967  data: 0.0002  max mem: 31303
Epoch: [4]  [1600/5004]  eta: 0:22:01  lr: 0.000043  min_lr: 0.000043  loss: 1.9268 (1.9721)  class_acc: 0.7500 (0.7533)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7499 (5.6103)  time: 0.3914  data: 0.0002  max mem: 31303
Epoch: [4]  [1800/5004]  eta: 0:20:43  lr: 0.000044  min_lr: 0.000044  loss: 1.9852 (1.9761)  class_acc: 0.7812 (0.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4827 (5.6588)  time: 0.3810  data: 0.0003  max mem: 31303
Epoch: [4]  [2000/5004]  eta: 0:19:25  lr: 0.000044  min_lr: 0.000044  loss: 1.9344 (1.9772)  class_acc: 0.7812 (0.7524)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2940 (5.6336)  time: 0.3813  data: 0.0002  max mem: 31303
Epoch: [4]  [2200/5004]  eta: 0:18:07  lr: 0.000044  min_lr: 0.000044  loss: 1.8841 (1.9784)  class_acc: 0.7812 (0.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9414 (5.6012)  time: 0.3897  data: 0.0002  max mem: 31303
Epoch: [4]  [2400/5004]  eta: 0:16:50  lr: 0.000045  min_lr: 0.000045  loss: 1.8054 (1.9777)  class_acc: 0.7812 (0.7513)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6594 (5.5588)  time: 0.3867  data: 0.0003  max mem: 31303
Epoch: [4]  [2600/5004]  eta: 0:15:33  lr: 0.000045  min_lr: 0.000045  loss: 1.9391 (1.9756)  class_acc: 0.7812 (0.7519)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6852 (5.5285)  time: 0.3869  data: 0.0002  max mem: 31303
Epoch: [4]  [2800/5004]  eta: 0:14:15  lr: 0.000046  min_lr: 0.000046  loss: 1.9661 (1.9731)  class_acc: 0.7500 (0.7524)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4461 (5.5319)  time: 0.3850  data: 0.0002  max mem: 31303
Epoch: [4]  [3000/5004]  eta: 0:12:58  lr: 0.000046  min_lr: 0.000046  loss: 1.9346 (1.9745)  class_acc: 0.7812 (0.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0587 (5.5202)  time: 0.3834  data: 0.0002  max mem: 31303
Epoch: [4]  [3200/5004]  eta: 0:11:40  lr: 0.000046  min_lr: 0.000046  loss: 1.9061 (1.9744)  class_acc: 0.7812 (0.7524)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0617 (5.5154)  time: 0.3883  data: 0.0002  max mem: 31303
Epoch: [4]  [3400/5004]  eta: 0:10:23  lr: 0.000047  min_lr: 0.000047  loss: 1.7860 (1.9733)  class_acc: 0.7188 (0.7524)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9460 (5.4669)  time: 0.3833  data: 0.0002  max mem: 31303
Epoch: [4]  [3600/5004]  eta: 0:09:05  lr: 0.000047  min_lr: 0.000047  loss: 1.8852 (1.9751)  class_acc: 0.7812 (0.7522)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0442 (5.4587)  time: 0.3849  data: 0.0002  max mem: 31303
Epoch: [4]  [3800/5004]  eta: 0:07:48  lr: 0.000048  min_lr: 0.000048  loss: 1.8783 (1.9747)  class_acc: 0.7812 (0.7523)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0176 (5.4502)  time: 0.3867  data: 0.0002  max mem: 31303
Epoch: [4]  [4000/5004]  eta: 0:06:30  lr: 0.000048  min_lr: 0.000048  loss: 2.0229 (1.9739)  class_acc: 0.7812 (0.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3334 (5.4502)  time: 0.3867  data: 0.0002  max mem: 31303
Epoch: [4]  [4200/5004]  eta: 0:05:12  lr: 0.000048  min_lr: 0.000048  loss: 1.8358 (1.9730)  class_acc: 0.7500 (0.7528)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5046 (5.4480)  time: 0.3816  data: 0.0002  max mem: 31303
Epoch: [4]  [4400/5004]  eta: 0:03:54  lr: 0.000049  min_lr: 0.000049  loss: 1.9055 (1.9723)  class_acc: 0.7500 (0.7530)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3736 (5.4564)  time: 0.3830  data: 0.0002  max mem: 31303
Epoch: [4]  [4600/5004]  eta: 0:02:37  lr: 0.000049  min_lr: 0.000049  loss: 1.9955 (1.9727)  class_acc: 0.7500 (0.7529)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9829 (5.4569)  time: 0.3858  data: 0.0002  max mem: 31303
Epoch: [4]  [4800/5004]  eta: 0:01:19  lr: 0.000050  min_lr: 0.000050  loss: 2.0108 (1.9729)  class_acc: 0.7500 (0.7530)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2210 (5.4305)  time: 0.3816  data: 0.0003  max mem: 31303
Epoch: [4]  [5000/5004]  eta: 0:00:01  lr: 0.000050  min_lr: 0.000050  loss: 1.8969 (1.9728)  class_acc: 0.7500 (0.7529)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0909 (5.4240)  time: 0.3779  data: 0.0008  max mem: 31303
Epoch: [4]  [5003/5004]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000050  loss: 1.8759 (1.9727)  class_acc: 0.7500 (0.7530)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1564 (5.4252)  time: 0.3763  data: 0.0007  max mem: 31303
Epoch: [4] Total time: 0:32:26 (0.3889 s / it)
Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 1.8759 (1.9744)  class_acc: 0.7500 (0.7526)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1564 (5.4252)
Test:  [ 0/25]  eta: 0:05:00  loss: 0.3897 (0.3897)  acc1: 91.2000 (91.2000)  acc5: 99.2000 (99.2000)  time: 12.0355  data: 11.3114  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5494 (0.5380)  acc1: 87.2000 (87.9273)  acc5: 97.6000 (97.8545)  time: 1.7279  data: 1.0286  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6343 (0.6218)  acc1: 85.2000 (85.2571)  acc5: 96.8000 (97.2762)  time: 0.6957  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6865 (0.6353)  acc1: 84.8000 (84.6400)  acc5: 96.8000 (97.2480)  time: 0.6943  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1551 s / it)
* Acc@1 85.204 Acc@5 97.336 loss 0.624
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.20%
Test:  [ 0/25]  eta: 0:04:34  loss: 0.5350 (0.5350)  acc1: 92.4000 (92.4000)  acc5: 99.6000 (99.6000)  time: 10.9788  data: 10.2736  max mem: 31303
Test:  [10/25]  eta: 0:00:24  loss: 0.6825 (0.6816)  acc1: 86.8000 (87.7818)  acc5: 98.0000 (98.2182)  time: 1.6288  data: 0.9342  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.7689 (0.7501)  acc1: 84.4000 (85.3524)  acc5: 97.2000 (97.5619)  time: 0.6938  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.7805 (0.7580)  acc1: 84.0000 (84.8480)  acc5: 97.2000 (97.4880)  time: 0.6938  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.1120 s / it)
* Acc@1 85.354 Acc@5 97.480 loss 0.744
Accuracy of the model EMA on 50000 test images: 85.4%
Max EMA accuracy: 85.35%
Epoch: [5]  [   0/5004]  eta: 1:41:55  lr: 0.000050  min_lr: 0.000050  loss: 2.5734 (2.5734)  class_acc: 0.5938 (0.5938)  weight_decay: 0.0500 (0.0500)  time: 1.2221  data: 0.8450  max mem: 31303
Epoch: [5]  [ 200/5004]  eta: 0:31:24  lr: 0.000050  min_lr: 0.000050  loss: 1.7776 (1.9577)  class_acc: 0.7500 (0.7564)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3312 (5.8288)  time: 0.3903  data: 0.0002  max mem: 31303
Epoch: [5]  [ 400/5004]  eta: 0:30:08  lr: 0.000050  min_lr: 0.000050  loss: 1.8875 (1.9629)  class_acc: 0.7812 (0.7566)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8224 (5.5005)  time: 0.3915  data: 0.0002  max mem: 31303
Epoch: [5]  [ 600/5004]  eta: 0:28:37  lr: 0.000050  min_lr: 0.000050  loss: 1.9743 (1.9579)  class_acc: 0.7812 (0.7560)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3319 (5.4392)  time: 0.3800  data: 0.0002  max mem: 31303
Epoch: [5]  [ 800/5004]  eta: 0:27:18  lr: 0.000050  min_lr: 0.000050  loss: 1.9338 (1.9556)  class_acc: 0.7500 (0.7564)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7305 (5.7094)  time: 0.3955  data: 0.0002  max mem: 31303
Epoch: [5]  [1000/5004]  eta: 0:25:59  lr: 0.000050  min_lr: 0.000050  loss: 2.1299 (1.9578)  class_acc: 0.6875 (0.7554)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2054 (5.4966)  time: 0.3837  data: 0.0002  max mem: 31303
Epoch: [5]  [1200/5004]  eta: 0:24:37  lr: 0.000050  min_lr: 0.000050  loss: 1.9303 (1.9624)  class_acc: 0.7500 (0.7551)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6684 (5.4975)  time: 0.3792  data: 0.0002  max mem: 31303
Epoch: [5]  [1400/5004]  eta: 0:23:17  lr: 0.000050  min_lr: 0.000050  loss: 2.0113 (1.9616)  class_acc: 0.7500 (0.7554)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2116 (5.5587)  time: 0.3841  data: 0.0002  max mem: 31303
Epoch: [5]  [1600/5004]  eta: 0:22:01  lr: 0.000050  min_lr: 0.000050  loss: 1.9258 (1.9619)  class_acc: 0.7500 (0.7554)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1235 (5.5231)  time: 0.3819  data: 0.0002  max mem: 31303
Epoch: [5]  [1800/5004]  eta: 0:20:42  lr: 0.000050  min_lr: 0.000050  loss: 1.8962 (1.9607)  class_acc: 0.7500 (0.7555)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8756 (5.5323)  time: 0.3827  data: 0.0002  max mem: 31303
Epoch: [5]  [2000/5004]  eta: 0:19:24  lr: 0.000050  min_lr: 0.000050  loss: 1.8816 (1.9622)  class_acc: 0.7500 (0.7548)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2311 (5.6206)  time: 0.3968  data: 0.0002  max mem: 31303
Epoch: [5]  [2200/5004]  eta: 0:18:06  lr: 0.000050  min_lr: 0.000050  loss: 1.7756 (1.9617)  class_acc: 0.8125 (0.7548)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0440 (5.5927)  time: 0.3811  data: 0.0002  max mem: 31303
Epoch: [5]  [2400/5004]  eta: 0:16:49  lr: 0.000050  min_lr: 0.000050  loss: 1.8267 (1.9628)  class_acc: 0.7812 (0.7544)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1527 (5.6044)  time: 0.3888  data: 0.0002  max mem: 31303
Epoch: [5]  [2600/5004]  eta: 0:15:32  lr: 0.000050  min_lr: 0.000050  loss: 1.8828 (1.9622)  class_acc: 0.7500 (0.7544)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4290 (5.5762)  time: 0.3849  data: 0.0002  max mem: 31303
Epoch: [5]  [2800/5004]  eta: 0:14:14  lr: 0.000050  min_lr: 0.000050  loss: 1.9316 (1.9620)  class_acc: 0.7500 (0.7545)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7944 (5.5172)  time: 0.3786  data: 0.0002  max mem: 31303
Epoch: [5]  [3000/5004]  eta: 0:12:56  lr: 0.000050  min_lr: 0.000050  loss: 1.7928 (1.9604)  class_acc: 0.8125 (0.7549)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9982 (5.5165)  time: 0.3896  data: 0.0002  max mem: 31303
Epoch: [5]  [3200/5004]  eta: 0:11:38  lr: 0.000050  min_lr: 0.000050  loss: 1.7602 (1.9602)  class_acc: 0.8125 (0.7549)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1114 (5.4969)  time: 0.3929  data: 0.0002  max mem: 31303
Epoch: [5]  [3400/5004]  eta: 0:10:20  lr: 0.000049  min_lr: 0.000049  loss: 1.9152 (1.9612)  class_acc: 0.7500 (0.7544)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9468 (5.4876)  time: 0.3902  data: 0.0002  max mem: 31303
Epoch: [5]  [3600/5004]  eta: 0:09:03  lr: 0.000049  min_lr: 0.000049  loss: 1.9788 (1.9604)  class_acc: 0.7812 (0.7548)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0247 (5.4772)  time: 0.3793  data: 0.0002  max mem: 31303
Epoch: [5]  [3800/5004]  eta: 0:07:45  lr: 0.000049  min_lr: 0.000049  loss: 1.8367 (1.9608)  class_acc: 0.7812 (0.7547)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2907 (5.4538)  time: 0.3817  data: 0.0002  max mem: 31303
Epoch: [5]  [4000/5004]  eta: 0:06:28  lr: 0.000049  min_lr: 0.000049  loss: 1.9549 (1.9619)  class_acc: 0.7500 (0.7543)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4973 (5.4372)  time: 0.4123  data: 0.0002  max mem: 31303
Epoch: [5]  [4200/5004]  eta: 0:05:10  lr: 0.000049  min_lr: 0.000049  loss: 1.9370 (1.9617)  class_acc: 0.7500 (0.7546)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8411 (5.4118)  time: 0.3786  data: 0.0002  max mem: 31303
Epoch: [5]  [4400/5004]  eta: 0:03:53  lr: 0.000049  min_lr: 0.000049  loss: 1.9513 (1.9613)  class_acc: 0.7500 (0.7546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0655 (5.4642)  time: 0.3845  data: 0.0002  max mem: 31303
Epoch: [5]  [4600/5004]  eta: 0:02:36  lr: 0.000049  min_lr: 0.000049  loss: 1.9091 (1.9606)  class_acc: 0.7812 (0.7546)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0500 (5.4567)  time: 0.3785  data: 0.0002  max mem: 31303
Epoch: [5]  [4800/5004]  eta: 0:01:18  lr: 0.000049  min_lr: 0.000049  loss: 1.8877 (1.9591)  class_acc: 0.7812 (0.7546)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5269 (5.4492)  time: 0.3813  data: 0.0002  max mem: 31303
Epoch: [5]  [5000/5004]  eta: 0:00:01  lr: 0.000049  min_lr: 0.000049  loss: 1.9764 (1.9597)  class_acc: 0.7188 (0.7544)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9081 (5.4292)  time: 0.3881  data: 0.0005  max mem: 31303
Epoch: [5]  [5003/5004]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000049  loss: 2.0315 (1.9598)  class_acc: 0.7188 (0.7544)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9081 (5.4272)  time: 0.3864  data: 0.0005  max mem: 31303
Epoch: [5] Total time: 0:32:14 (0.3867 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 2.0315 (1.9597)  class_acc: 0.7188 (0.7552)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9081 (5.4272)
Test:  [ 0/25]  eta: 0:03:25  loss: 0.3870 (0.3870)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 8.2094  data: 7.4860  max mem: 31303
Test:  [10/25]  eta: 0:00:22  loss: 0.5617 (0.5430)  acc1: 86.4000 (87.6727)  acc5: 97.6000 (97.9636)  time: 1.4868  data: 0.7911  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6418 (0.6234)  acc1: 84.8000 (85.2571)  acc5: 97.2000 (97.3714)  time: 0.7537  data: 0.0609  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6810 (0.6372)  acc1: 84.4000 (84.6880)  acc5: 96.8000 (97.2640)  time: 0.6930  data: 0.0001  max mem: 31303
Test: Total time: 0:00:26 (1.0490 s / it)
* Acc@1 85.080 Acc@5 97.386 loss 0.625
Accuracy of the model on the 50000 test images: 85.1%
Max accuracy: 85.20%
Test:  [ 0/25]  eta: 0:03:44  loss: 0.4921 (0.4921)  acc1: 92.0000 (92.0000)  acc5: 99.6000 (99.6000)  time: 8.9872  data: 8.2851  max mem: 31303
Test:  [10/25]  eta: 0:00:21  loss: 0.6411 (0.6389)  acc1: 86.8000 (87.8182)  acc5: 98.0000 (98.1455)  time: 1.4478  data: 0.7536  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.7347 (0.7111)  acc1: 84.0000 (85.4095)  acc5: 97.2000 (97.5429)  time: 0.6932  data: 0.0003  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.7349 (0.7203)  acc1: 84.0000 (84.8640)  acc5: 97.2000 (97.5040)  time: 0.6930  data: 0.0002  max mem: 31303
Test: Total time: 0:00:25 (1.0320 s / it)
* Acc@1 85.412 Acc@5 97.490 loss 0.707
Accuracy of the model EMA on 50000 test images: 85.4%
Max EMA accuracy: 85.41%
Epoch: [6]  [   0/5004]  eta: 2:03:57  lr: 0.000049  min_lr: 0.000049  loss: 2.5659 (2.5659)  class_acc: 0.6250 (0.6250)  weight_decay: 0.0500 (0.0500)  time: 1.4862  data: 1.1100  max mem: 31303
Epoch: [6]  [ 200/5004]  eta: 0:31:17  lr: 0.000049  min_lr: 0.000049  loss: 1.8765 (1.9513)  class_acc: 0.7500 (0.7575)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2957 (4.3188)  time: 0.4172  data: 0.0002  max mem: 31303
Epoch: [6]  [ 400/5004]  eta: 0:29:44  lr: 0.000049  min_lr: 0.000049  loss: 1.9971 (1.9401)  class_acc: 0.7500 (0.7590)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2944 (4.7996)  time: 0.4021  data: 0.0002  max mem: 31303
Epoch: [6]  [ 600/5004]  eta: 0:28:30  lr: 0.000048  min_lr: 0.000048  loss: 2.0194 (1.9526)  class_acc: 0.7188 (0.7545)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4294 (4.9236)  time: 0.4032  data: 0.0002  max mem: 31303
Epoch: [6]  [ 800/5004]  eta: 0:27:10  lr: 0.000048  min_lr: 0.000048  loss: 1.8581 (1.9508)  class_acc: 0.7500 (0.7568)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0430 (5.0216)  time: 0.3991  data: 0.0002  max mem: 31303
Epoch: [6]  [1000/5004]  eta: 0:25:51  lr: 0.000048  min_lr: 0.000048  loss: 1.9099 (1.9494)  class_acc: 0.7500 (0.7575)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2339 (5.2144)  time: 0.3873  data: 0.0002  max mem: 31303
Epoch: [6]  [1200/5004]  eta: 0:24:34  lr: 0.000048  min_lr: 0.000048  loss: 1.7791 (1.9529)  class_acc: 0.7500 (0.7575)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4951 (5.4143)  time: 0.3857  data: 0.0002  max mem: 31303
Epoch: [6]  [1400/5004]  eta: 0:23:16  lr: 0.000048  min_lr: 0.000048  loss: 1.7645 (1.9510)  class_acc: 0.7812 (0.7569)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6135 (5.5447)  time: 0.3837  data: 0.0002  max mem: 31303
Epoch: [6]  [1600/5004]  eta: 0:21:58  lr: 0.000048  min_lr: 0.000048  loss: 1.8664 (1.9499)  class_acc: 0.7812 (0.7574)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7569 (5.4834)  time: 0.3811  data: 0.0002  max mem: 31303
Epoch: [6]  [1800/5004]  eta: 0:20:40  lr: 0.000048  min_lr: 0.000048  loss: 1.9845 (1.9521)  class_acc: 0.7500 (0.7568)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2748 (5.4111)  time: 0.3829  data: 0.0002  max mem: 31303
Epoch: [6]  [2000/5004]  eta: 0:19:22  lr: 0.000048  min_lr: 0.000048  loss: 2.0571 (1.9541)  class_acc: 0.7188 (0.7562)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1753 (5.3831)  time: 0.3870  data: 0.0002  max mem: 31303
Epoch: [6]  [2200/5004]  eta: 0:18:06  lr: 0.000048  min_lr: 0.000048  loss: 1.8942 (1.9531)  class_acc: 0.7500 (0.7567)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3660 (5.3798)  time: 0.3875  data: 0.0002  max mem: 31303
Epoch: [6]  [2400/5004]  eta: 0:16:49  lr: 0.000047  min_lr: 0.000047  loss: 1.9883 (1.9541)  class_acc: 0.7500 (0.7562)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3258 (5.3160)  time: 0.3886  data: 0.0002  max mem: 31303
Epoch: [6]  [2600/5004]  eta: 0:15:32  lr: 0.000047  min_lr: 0.000047  loss: 1.8624 (1.9541)  class_acc: 0.7500 (0.7563)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2844 (5.2835)  time: 0.3823  data: 0.0002  max mem: 31303
Epoch: [6]  [2800/5004]  eta: 0:14:14  lr: 0.000047  min_lr: 0.000047  loss: 1.8880 (1.9519)  class_acc: 0.7500 (0.7570)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1413 (5.2758)  time: 0.3887  data: 0.0002  max mem: 31303
Epoch: [6]  [3000/5004]  eta: 0:12:57  lr: 0.000047  min_lr: 0.000047  loss: 1.9537 (1.9525)  class_acc: 0.7500 (0.7567)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2180 (5.2289)  time: 0.3836  data: 0.0002  max mem: 31303
Epoch: [6]  [3200/5004]  eta: 0:11:39  lr: 0.000047  min_lr: 0.000047  loss: 1.9312 (1.9521)  class_acc: 0.7500 (0.7566)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3008 (5.2119)  time: 0.3817  data: 0.0003  max mem: 31303
Epoch: [6]  [3400/5004]  eta: 0:10:21  lr: 0.000047  min_lr: 0.000047  loss: 1.9582 (1.9525)  class_acc: 0.7500 (0.7564)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2125 (5.2027)  time: 0.3887  data: 0.0002  max mem: 31303
Epoch: [6]  [3600/5004]  eta: 0:09:04  lr: 0.000047  min_lr: 0.000047  loss: 1.9777 (1.9521)  class_acc: 0.7500 (0.7564)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6245 (5.1849)  time: 0.3902  data: 0.0002  max mem: 31303
Epoch: [6]  [3800/5004]  eta: 0:07:47  lr: 0.000046  min_lr: 0.000046  loss: 1.8759 (1.9522)  class_acc: 0.7188 (0.7565)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2412 (5.1849)  time: 0.3895  data: 0.0002  max mem: 31303
Epoch: [6]  [4000/5004]  eta: 0:06:29  lr: 0.000046  min_lr: 0.000046  loss: 2.0155 (1.9529)  class_acc: 0.7500 (0.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9377 (5.1928)  time: 0.3908  data: 0.0002  max mem: 31303
Epoch: [6]  [4200/5004]  eta: 0:05:12  lr: 0.000046  min_lr: 0.000046  loss: 1.8894 (1.9517)  class_acc: 0.7500 (0.7563)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3572 (5.2101)  time: 0.3818  data: 0.0002  max mem: 31303
Epoch: [6]  [4400/5004]  eta: 0:03:54  lr: 0.000046  min_lr: 0.000046  loss: 1.8513 (1.9523)  class_acc: 0.7812 (0.7559)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4544 (5.1970)  time: 0.3864  data: 0.0002  max mem: 31303
Epoch: [6]  [4600/5004]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000046  loss: 1.9732 (1.9518)  class_acc: 0.7500 (0.7560)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0321 (5.1586)  time: 0.3906  data: 0.0002  max mem: 31303
Epoch: [6]  [4800/5004]  eta: 0:01:19  lr: 0.000046  min_lr: 0.000046  loss: 1.8546 (1.9509)  class_acc: 0.8125 (0.7563)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3322 (5.1456)  time: 0.3830  data: 0.0002  max mem: 31303
Epoch: [6]  [5000/5004]  eta: 0:00:01  lr: 0.000045  min_lr: 0.000045  loss: 1.9383 (1.9500)  class_acc: 0.7500 (0.7566)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7073 (5.1356)  time: 0.3857  data: 0.0006  max mem: 31303
Epoch: [6]  [5003/5004]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000045  loss: 1.9222 (1.9500)  class_acc: 0.7500 (0.7566)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7993 (5.1349)  time: 0.3848  data: 0.0006  max mem: 31303
Epoch: [6] Total time: 0:32:25 (0.3888 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000045  loss: 1.9222 (1.9463)  class_acc: 0.7500 (0.7587)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7993 (5.1349)
Test:  [ 0/25]  eta: 0:04:27  loss: 0.3903 (0.3903)  acc1: 90.8000 (90.8000)  acc5: 99.6000 (99.6000)  time: 10.6878  data: 9.9746  max mem: 31303
Test:  [10/25]  eta: 0:00:24  loss: 0.5490 (0.5364)  acc1: 86.8000 (87.7818)  acc5: 97.6000 (97.9273)  time: 1.6149  data: 0.9071  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6367 (0.6170)  acc1: 84.4000 (85.0857)  acc5: 96.8000 (97.4476)  time: 0.7011  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6771 (0.6300)  acc1: 84.0000 (84.5440)  acc5: 96.8000 (97.3440)  time: 0.6973  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.1078 s / it)
* Acc@1 85.158 Acc@5 97.342 loss 0.622
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.20%
Test:  [ 0/25]  eta: 0:04:57  loss: 0.4625 (0.4625)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.9028  data: 11.1851  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.6128 (0.6089)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1455)  time: 1.7134  data: 1.0171  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.7034 (0.6834)  acc1: 84.8000 (85.5048)  acc5: 97.2000 (97.5810)  time: 0.6939  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.7105 (0.6936)  acc1: 84.4000 (84.9600)  acc5: 97.2000 (97.5360)  time: 0.6936  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1494 s / it)
* Acc@1 85.454 Acc@5 97.508 loss 0.681
Accuracy of the model EMA on 50000 test images: 85.5%
Max EMA accuracy: 85.45%
Epoch: [7]  [   0/5004]  eta: 1:43:41  lr: 0.000045  min_lr: 0.000045  loss: 2.1644 (2.1644)  class_acc: 0.7500 (0.7500)  weight_decay: 0.0500 (0.0500)  time: 1.2434  data: 0.8839  max mem: 31303
Epoch: [7]  [ 200/5004]  eta: 0:31:34  lr: 0.000045  min_lr: 0.000045  loss: 1.8899 (1.9305)  class_acc: 0.7812 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3427 (5.5883)  time: 0.3867  data: 0.0002  max mem: 31303
Epoch: [7]  [ 400/5004]  eta: 0:30:05  lr: 0.000045  min_lr: 0.000045  loss: 2.0109 (1.9340)  class_acc: 0.7188 (0.7653)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7623 (5.6756)  time: 0.3891  data: 0.0002  max mem: 31303
Epoch: [7]  [ 600/5004]  eta: 0:28:39  lr: 0.000045  min_lr: 0.000045  loss: 1.7762 (1.9265)  class_acc: 0.7500 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7739 (5.5572)  time: 0.3859  data: 0.0002  max mem: 31303
Epoch: [7]  [ 800/5004]  eta: 0:27:19  lr: 0.000045  min_lr: 0.000045  loss: 2.0731 (1.9361)  class_acc: 0.7500 (0.7644)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5536 (5.6244)  time: 0.3842  data: 0.0002  max mem: 31303
Epoch: [7]  [1000/5004]  eta: 0:26:00  lr: 0.000044  min_lr: 0.000044  loss: 1.8532 (1.9357)  class_acc: 0.7812 (0.7640)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5353 (5.6104)  time: 0.3860  data: 0.0002  max mem: 31303
Epoch: [7]  [1200/5004]  eta: 0:24:44  lr: 0.000044  min_lr: 0.000044  loss: 1.9099 (1.9343)  class_acc: 0.7500 (0.7631)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8851 (5.4354)  time: 0.4000  data: 0.0002  max mem: 31303
Epoch: [7]  [1400/5004]  eta: 0:23:24  lr: 0.000044  min_lr: 0.000044  loss: 1.8584 (1.9334)  class_acc: 0.8125 (0.7637)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0147 (5.4715)  time: 0.3941  data: 0.0002  max mem: 31303
Epoch: [7]  [1600/5004]  eta: 0:22:05  lr: 0.000044  min_lr: 0.000044  loss: 1.9126 (1.9290)  class_acc: 0.7812 (0.7642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2712 (5.5034)  time: 0.3944  data: 0.0002  max mem: 31303
Epoch: [7]  [1800/5004]  eta: 0:20:47  lr: 0.000044  min_lr: 0.000044  loss: 1.9367 (1.9339)  class_acc: 0.7500 (0.7630)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8457 (5.4741)  time: 0.3982  data: 0.0003  max mem: 31303
Epoch: [7]  [2000/5004]  eta: 0:19:28  lr: 0.000043  min_lr: 0.000043  loss: 1.8722 (1.9333)  class_acc: 0.7500 (0.7628)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1636 (5.4374)  time: 0.3912  data: 0.0002  max mem: 31303
Epoch: [7]  [2200/5004]  eta: 0:18:09  lr: 0.000043  min_lr: 0.000043  loss: 1.9023 (1.9362)  class_acc: 0.7500 (0.7616)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9342 (5.3805)  time: 0.3818  data: 0.0002  max mem: 31303
Epoch: [7]  [2400/5004]  eta: 0:16:51  lr: 0.000043  min_lr: 0.000043  loss: 1.8843 (1.9363)  class_acc: 0.7500 (0.7609)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8622 (5.3263)  time: 0.3811  data: 0.0002  max mem: 31303
Epoch: [7]  [2600/5004]  eta: 0:15:33  lr: 0.000043  min_lr: 0.000043  loss: 1.9376 (1.9344)  class_acc: 0.7500 (0.7615)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0537 (5.2784)  time: 0.3817  data: 0.0003  max mem: 31303
Epoch: [7]  [2800/5004]  eta: 0:14:16  lr: 0.000042  min_lr: 0.000042  loss: 1.7826 (1.9370)  class_acc: 0.7500 (0.7609)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6944 (5.2647)  time: 0.4086  data: 0.0002  max mem: 31303
Epoch: [7]  [3000/5004]  eta: 0:12:57  lr: 0.000042  min_lr: 0.000042  loss: 1.9330 (1.9386)  class_acc: 0.7500 (0.7606)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6843 (5.1785)  time: 0.3817  data: 0.0002  max mem: 31303
Epoch: [7]  [3200/5004]  eta: 0:11:40  lr: 0.000042  min_lr: 0.000042  loss: 1.7206 (1.9379)  class_acc: 0.7812 (0.7606)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9357 (5.1297)  time: 0.3845  data: 0.0002  max mem: 31303
Epoch: [7]  [3400/5004]  eta: 0:10:22  lr: 0.000042  min_lr: 0.000042  loss: 1.8740 (1.9364)  class_acc: 0.7500 (0.7608)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8486 (5.1057)  time: 0.3832  data: 0.0003  max mem: 31303
Epoch: [7]  [3600/5004]  eta: 0:09:05  lr: 0.000042  min_lr: 0.000042  loss: 1.8275 (1.9360)  class_acc: 0.7500 (0.7606)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5168 (5.0854)  time: 0.4184  data: 0.0002  max mem: 31303
Epoch: [7]  [3800/5004]  eta: 0:07:47  lr: 0.000041  min_lr: 0.000041  loss: 1.8985 (1.9363)  class_acc: 0.7812 (0.7608)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8563 (5.0769)  time: 0.3821  data: 0.0002  max mem: 31303
Epoch: [7]  [4000/5004]  eta: 0:06:29  lr: 0.000041  min_lr: 0.000041  loss: 1.9800 (1.9372)  class_acc: 0.7188 (0.7606)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1723 (inf)  time: 0.3872  data: 0.0002  max mem: 31303
Epoch: [7]  [4200/5004]  eta: 0:05:12  lr: 0.000041  min_lr: 0.000041  loss: 1.8915 (1.9378)  class_acc: 0.7500 (0.7605)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5916 (inf)  time: 0.3850  data: 0.0002  max mem: 31303
Epoch: [7]  [4400/5004]  eta: 0:03:54  lr: 0.000041  min_lr: 0.000041  loss: 1.8457 (1.9378)  class_acc: 0.7500 (0.7603)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0429 (inf)  time: 0.4197  data: 0.0002  max mem: 31303
Epoch: [7]  [4600/5004]  eta: 0:02:36  lr: 0.000040  min_lr: 0.000040  loss: 1.9805 (1.9374)  class_acc: 0.7188 (0.7602)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7329 (inf)  time: 0.3819  data: 0.0002  max mem: 31303
Epoch: [7]  [4800/5004]  eta: 0:01:19  lr: 0.000040  min_lr: 0.000040  loss: 2.0136 (1.9380)  class_acc: 0.7500 (0.7601)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2749 (inf)  time: 0.3815  data: 0.0002  max mem: 31303
Epoch: [7]  [5000/5004]  eta: 0:00:01  lr: 0.000040  min_lr: 0.000040  loss: 1.7695 (1.9381)  class_acc: 0.7500 (0.7600)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0902 (inf)  time: 0.3778  data: 0.0006  max mem: 31303
Epoch: [7]  [5003/5004]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000040  loss: 1.9006 (1.9383)  class_acc: 0.7500 (0.7600)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0935 (inf)  time: 0.3766  data: 0.0006  max mem: 31303
Epoch: [7] Total time: 0:32:23 (0.3884 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000040  loss: 1.9006 (1.9378)  class_acc: 0.7500 (0.7606)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0935 (inf)
Test:  [ 0/25]  eta: 0:04:51  loss: 0.3728 (0.3728)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.6767  data: 10.9499  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5518 (0.5299)  acc1: 87.2000 (87.7818)  acc5: 97.6000 (98.0364)  time: 1.6941  data: 0.9957  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6180 (0.6121)  acc1: 84.4000 (85.0667)  acc5: 97.2000 (97.4095)  time: 0.6951  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6690 (0.6246)  acc1: 83.6000 (84.6080)  acc5: 96.8000 (97.3120)  time: 0.6943  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1417 s / it)
* Acc@1 85.106 Acc@5 97.348 loss 0.618
Accuracy of the model on the 50000 test images: 85.1%
Max accuracy: 85.20%
Test:  [ 0/25]  eta: 0:04:51  loss: 0.4411 (0.4411)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.6518  data: 10.9120  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5923 (0.5869)  acc1: 86.8000 (87.9273)  acc5: 98.0000 (98.1818)  time: 1.6907  data: 0.9922  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6810 (0.6629)  acc1: 85.2000 (85.5429)  acc5: 97.2000 (97.5810)  time: 0.6943  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6927 (0.6737)  acc1: 85.2000 (85.0560)  acc5: 97.2000 (97.5520)  time: 0.6940  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1421 s / it)
* Acc@1 85.494 Acc@5 97.528 loss 0.662
Accuracy of the model EMA on 50000 test images: 85.5%
Max EMA accuracy: 85.49%
Epoch: [8]  [   0/5004]  eta: 1:47:59  lr: 0.000040  min_lr: 0.000040  loss: 2.3496 (2.3496)  class_acc: 0.5938 (0.5938)  weight_decay: 0.0500 (0.0500)  time: 1.2948  data: 0.9300  max mem: 31303
Epoch: [8]  [ 200/5004]  eta: 0:31:21  lr: 0.000040  min_lr: 0.000040  loss: 1.8783 (1.9438)  class_acc: 0.7812 (0.7629)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0462 (4.5474)  time: 0.3815  data: 0.0002  max mem: 31303
Epoch: [8]  [ 400/5004]  eta: 0:29:50  lr: 0.000039  min_lr: 0.000039  loss: 1.9813 (1.9295)  class_acc: 0.7188 (0.7669)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0821 (4.5537)  time: 0.3830  data: 0.0002  max mem: 31303
Epoch: [8]  [ 600/5004]  eta: 0:28:31  lr: 0.000039  min_lr: 0.000039  loss: 1.9638 (1.9327)  class_acc: 0.7500 (0.7644)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6582 (4.7152)  time: 0.3839  data: 0.0002  max mem: 31303
Epoch: [8]  [ 800/5004]  eta: 0:27:13  lr: 0.000039  min_lr: 0.000039  loss: 1.8659 (1.9278)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7536 (4.8425)  time: 0.3864  data: 0.0002  max mem: 31303
Epoch: [8]  [1000/5004]  eta: 0:25:56  lr: 0.000039  min_lr: 0.000039  loss: 1.9807 (1.9286)  class_acc: 0.7188 (0.7637)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4947 (4.7061)  time: 0.3826  data: 0.0002  max mem: 31303
Epoch: [8]  [1200/5004]  eta: 0:24:39  lr: 0.000038  min_lr: 0.000038  loss: 2.0826 (1.9334)  class_acc: 0.7188 (0.7621)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4583 (4.8140)  time: 0.3843  data: 0.0002  max mem: 31303
Epoch: [8]  [1400/5004]  eta: 0:23:20  lr: 0.000038  min_lr: 0.000038  loss: 1.8452 (1.9299)  class_acc: 0.7500 (0.7626)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2486 (4.8298)  time: 0.3822  data: 0.0002  max mem: 31303
Epoch: [8]  [1600/5004]  eta: 0:22:00  lr: 0.000038  min_lr: 0.000038  loss: 1.8307 (1.9298)  class_acc: 0.7500 (0.7623)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6788 (4.8657)  time: 0.3788  data: 0.0002  max mem: 31303
Epoch: [8]  [1800/5004]  eta: 0:20:42  lr: 0.000038  min_lr: 0.000038  loss: 1.8917 (1.9288)  class_acc: 0.7812 (0.7629)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1232 (5.0014)  time: 0.3846  data: 0.0002  max mem: 31303
Epoch: [8]  [2000/5004]  eta: 0:19:25  lr: 0.000037  min_lr: 0.000037  loss: 1.9567 (1.9320)  class_acc: 0.7500 (0.7622)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8977 (4.9820)  time: 0.3865  data: 0.0002  max mem: 31303
Epoch: [8]  [2200/5004]  eta: 0:18:07  lr: 0.000037  min_lr: 0.000037  loss: 1.9058 (1.9304)  class_acc: 0.7500 (0.7631)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5552 (5.0591)  time: 0.3814  data: 0.0002  max mem: 31303
Epoch: [8]  [2400/5004]  eta: 0:16:49  lr: 0.000037  min_lr: 0.000037  loss: 1.8927 (1.9280)  class_acc: 0.7500 (0.7633)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4062 (5.0473)  time: 0.3825  data: 0.0002  max mem: 31303
Epoch: [8]  [2600/5004]  eta: 0:15:31  lr: 0.000036  min_lr: 0.000036  loss: 1.9339 (1.9288)  class_acc: 0.7500 (0.7631)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4454 (5.0675)  time: 0.3863  data: 0.0002  max mem: 31303
Epoch: [8]  [2800/5004]  eta: 0:14:14  lr: 0.000036  min_lr: 0.000036  loss: 1.9170 (1.9295)  class_acc: 0.7500 (0.7629)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1315 (4.9969)  time: 0.3810  data: 0.0002  max mem: 31303
Epoch: [8]  [3000/5004]  eta: 0:12:56  lr: 0.000036  min_lr: 0.000036  loss: 1.8719 (1.9303)  class_acc: 0.7500 (0.7628)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4456 (4.9744)  time: 0.3889  data: 0.0002  max mem: 31303
Epoch: [8]  [3200/5004]  eta: 0:11:39  lr: 0.000036  min_lr: 0.000036  loss: 1.9227 (1.9300)  class_acc: 0.7812 (0.7628)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6974 (5.0313)  time: 0.3913  data: 0.0002  max mem: 31303
Epoch: [8]  [3400/5004]  eta: 0:10:21  lr: 0.000035  min_lr: 0.000035  loss: 1.7545 (1.9291)  class_acc: 0.7812 (0.7630)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5452 (5.0780)  time: 0.4142  data: 0.0002  max mem: 31303
Epoch: [8]  [3600/5004]  eta: 0:09:04  lr: 0.000035  min_lr: 0.000035  loss: 1.9237 (1.9303)  class_acc: 0.7500 (0.7627)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8119 (5.0867)  time: 0.4041  data: 0.0002  max mem: 31303
Epoch: [8]  [3800/5004]  eta: 0:07:46  lr: 0.000035  min_lr: 0.000035  loss: 1.8941 (1.9300)  class_acc: 0.7500 (0.7627)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8426 (5.0844)  time: 0.3890  data: 0.0002  max mem: 31303
Epoch: [8]  [4000/5004]  eta: 0:06:29  lr: 0.000035  min_lr: 0.000035  loss: 1.9205 (1.9314)  class_acc: 0.7812 (0.7622)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9441 (5.0857)  time: 0.3881  data: 0.0002  max mem: 31303
Epoch: [8]  [4200/5004]  eta: 0:05:12  lr: 0.000034  min_lr: 0.000034  loss: 1.8699 (1.9319)  class_acc: 0.7500 (0.7621)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1077 (nan)  time: 0.3912  data: 0.0002  max mem: 31303
Epoch: [8]  [4400/5004]  eta: 0:03:54  lr: 0.000034  min_lr: 0.000034  loss: 1.8773 (1.9294)  class_acc: 0.7812 (0.7625)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9501 (nan)  time: 0.3794  data: 0.0002  max mem: 31303
Epoch: [8]  [4600/5004]  eta: 0:02:36  lr: 0.000034  min_lr: 0.000034  loss: 1.9136 (1.9283)  class_acc: 0.7500 (0.7625)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8821 (nan)  time: 0.3896  data: 0.0002  max mem: 31303
Epoch: [8]  [4800/5004]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000033  loss: 1.8643 (1.9281)  class_acc: 0.7500 (0.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2891 (nan)  time: 0.3882  data: 0.0002  max mem: 31303
Epoch: [8]  [5000/5004]  eta: 0:00:01  lr: 0.000033  min_lr: 0.000033  loss: 1.8820 (1.9276)  class_acc: 0.7812 (0.7626)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1257 (nan)  time: 0.3831  data: 0.0004  max mem: 31303
Epoch: [8]  [5003/5004]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000033  loss: 1.8821 (1.9278)  class_acc: 0.7500 (0.7626)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1257 (nan)  time: 0.3824  data: 0.0004  max mem: 31303
Epoch: [8] Total time: 0:32:24 (0.3886 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000033  loss: 1.8821 (1.9279)  class_acc: 0.7500 (0.7632)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1257 (nan)
Test:  [ 0/25]  eta: 0:05:01  loss: 0.3898 (0.3898)  acc1: 91.2000 (91.2000)  acc5: 99.2000 (99.2000)  time: 12.0579  data: 11.3516  max mem: 31303
Test:  [10/25]  eta: 0:00:26  loss: 0.5507 (0.5354)  acc1: 86.8000 (87.8909)  acc5: 97.6000 (97.9636)  time: 1.7488  data: 1.0322  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6317 (0.6179)  acc1: 84.8000 (85.2952)  acc5: 97.2000 (97.4857)  time: 0.7146  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6743 (0.6311)  acc1: 84.0000 (84.7200)  acc5: 97.2000 (97.3920)  time: 0.7124  data: 0.0001  max mem: 31303
Test: Total time: 0:00:29 (1.1989 s / it)
* Acc@1 85.238 Acc@5 97.358 loss 0.623
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.24%
Test:  [ 0/25]  eta: 0:04:54  loss: 0.4260 (0.4260)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.7979  data: 11.0738  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5776 (0.5707)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1818)  time: 1.7021  data: 1.0069  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6640 (0.6477)  acc1: 85.2000 (85.5238)  acc5: 97.2000 (97.5810)  time: 0.6926  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6808 (0.6589)  acc1: 84.8000 (85.0240)  acc5: 97.2000 (97.5360)  time: 0.6926  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1434 s / it)
* Acc@1 85.536 Acc@5 97.526 loss 0.648
Accuracy of the model EMA on 50000 test images: 85.5%
Max EMA accuracy: 85.54%
Epoch: [9]  [   0/5004]  eta: 1:45:12  lr: 0.000033  min_lr: 0.000033  loss: 1.6798 (1.6798)  class_acc: 0.8438 (0.8438)  weight_decay: 0.0500 (0.0500)  time: 1.2615  data: 0.8878  max mem: 31303
Epoch: [9]  [ 200/5004]  eta: 0:31:31  lr: 0.000033  min_lr: 0.000033  loss: 1.8580 (1.9268)  class_acc: 0.7812 (0.7669)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9960 (4.7602)  time: 0.3829  data: 0.0002  max mem: 31303
Epoch: [9]  [ 400/5004]  eta: 0:29:58  lr: 0.000032  min_lr: 0.000032  loss: 1.8601 (1.9303)  class_acc: 0.7500 (0.7641)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7199 (4.6492)  time: 0.3810  data: 0.0002  max mem: 31303
Epoch: [9]  [ 600/5004]  eta: 0:28:33  lr: 0.000032  min_lr: 0.000032  loss: 1.9035 (1.9314)  class_acc: 0.7812 (0.7644)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7823 (4.9889)  time: 0.3807  data: 0.0002  max mem: 31303
Epoch: [9]  [ 800/5004]  eta: 0:27:14  lr: 0.000032  min_lr: 0.000032  loss: 1.8023 (1.9172)  class_acc: 0.7812 (0.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8808 (4.8811)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [9]  [1000/5004]  eta: 0:25:55  lr: 0.000032  min_lr: 0.000032  loss: 1.8658 (1.9147)  class_acc: 0.7500 (0.7671)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3735 (4.9132)  time: 0.3825  data: 0.0002  max mem: 31303
Epoch: [9]  [1200/5004]  eta: 0:24:37  lr: 0.000031  min_lr: 0.000031  loss: 1.9070 (1.9116)  class_acc: 0.7812 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5563 (4.9293)  time: 0.3857  data: 0.0002  max mem: 31303
Epoch: [9]  [1400/5004]  eta: 0:23:20  lr: 0.000031  min_lr: 0.000031  loss: 1.9278 (1.9160)  class_acc: 0.7188 (0.7671)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0869 (5.0043)  time: 0.3946  data: 0.0002  max mem: 31303
Epoch: [9]  [1600/5004]  eta: 0:22:02  lr: 0.000031  min_lr: 0.000031  loss: 1.9187 (1.9177)  class_acc: 0.7500 (0.7667)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6996 (5.0331)  time: 0.3932  data: 0.0002  max mem: 31303
Epoch: [9]  [1800/5004]  eta: 0:20:44  lr: 0.000030  min_lr: 0.000030  loss: 1.8109 (1.9170)  class_acc: 0.7812 (0.7669)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8490 (5.0660)  time: 0.3877  data: 0.0002  max mem: 31303
Epoch: [9]  [2000/5004]  eta: 0:19:27  lr: 0.000030  min_lr: 0.000030  loss: 1.9476 (1.9181)  class_acc: 0.7812 (0.7664)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9163 (5.0447)  time: 0.3991  data: 0.0002  max mem: 31303
Epoch: [9]  [2200/5004]  eta: 0:18:11  lr: 0.000030  min_lr: 0.000030  loss: 1.9668 (1.9166)  class_acc: 0.7500 (0.7663)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7317 (5.0355)  time: 0.3892  data: 0.0002  max mem: 31303
Epoch: [9]  [2400/5004]  eta: 0:16:54  lr: 0.000029  min_lr: 0.000029  loss: 1.9076 (1.9183)  class_acc: 0.7500 (0.7656)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8048 (5.0133)  time: 0.3881  data: 0.0002  max mem: 31303
Epoch: [9]  [2600/5004]  eta: 0:15:36  lr: 0.000029  min_lr: 0.000029  loss: 1.9777 (1.9172)  class_acc: 0.7500 (0.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6385 (4.9857)  time: 0.3886  data: 0.0002  max mem: 31303
Epoch: [9]  [2800/5004]  eta: 0:14:19  lr: 0.000029  min_lr: 0.000029  loss: 1.9163 (1.9187)  class_acc: 0.7500 (0.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8619 (5.0055)  time: 0.3888  data: 0.0002  max mem: 31303
Epoch: [9]  [3000/5004]  eta: 0:13:01  lr: 0.000029  min_lr: 0.000029  loss: 1.9298 (1.9197)  class_acc: 0.7812 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7515 (5.0032)  time: 0.3884  data: 0.0002  max mem: 31303
Epoch: [9]  [3200/5004]  eta: 0:11:43  lr: 0.000028  min_lr: 0.000028  loss: 1.8651 (1.9189)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3554 (5.0155)  time: 0.3955  data: 0.0002  max mem: 31303
Epoch: [9]  [3400/5004]  eta: 0:10:25  lr: 0.000028  min_lr: 0.000028  loss: 1.8217 (1.9187)  class_acc: 0.7812 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8401 (4.9993)  time: 0.4085  data: 0.0002  max mem: 31303
Epoch: [9]  [3600/5004]  eta: 0:09:07  lr: 0.000028  min_lr: 0.000028  loss: 1.8773 (1.9181)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8833 (5.0193)  time: 0.4031  data: 0.0002  max mem: 31303
Epoch: [9]  [3800/5004]  eta: 0:07:49  lr: 0.000027  min_lr: 0.000027  loss: 1.7922 (1.9177)  class_acc: 0.7812 (0.7652)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9046 (5.0252)  time: 0.4181  data: 0.0002  max mem: 31303
Epoch: [9]  [4000/5004]  eta: 0:06:31  lr: 0.000027  min_lr: 0.000027  loss: 1.9559 (1.9196)  class_acc: 0.7500 (0.7648)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8101 (5.0323)  time: 0.4180  data: 0.0002  max mem: 31303
Epoch: [9]  [4200/5004]  eta: 0:05:13  lr: 0.000027  min_lr: 0.000027  loss: 1.9018 (1.9190)  class_acc: 0.7500 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1174 (4.9981)  time: 0.4127  data: 0.0002  max mem: 31303
Epoch: [9]  [4400/5004]  eta: 0:03:55  lr: 0.000026  min_lr: 0.000026  loss: 1.8106 (1.9195)  class_acc: 0.7812 (0.7649)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8414 (4.9888)  time: 0.4012  data: 0.0002  max mem: 31303
Epoch: [9]  [4600/5004]  eta: 0:02:37  lr: 0.000026  min_lr: 0.000026  loss: 1.9253 (1.9186)  class_acc: 0.7500 (0.7653)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8598 (4.9631)  time: 0.4032  data: 0.0002  max mem: 31303
Epoch: [9]  [4800/5004]  eta: 0:01:19  lr: 0.000026  min_lr: 0.000026  loss: 1.8107 (1.9189)  class_acc: 0.7500 (0.7652)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3366 (4.9448)  time: 0.3820  data: 0.0002  max mem: 31303
Epoch: [9]  [5000/5004]  eta: 0:00:01  lr: 0.000026  min_lr: 0.000026  loss: 1.9036 (1.9185)  class_acc: 0.7812 (0.7653)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8320 (4.9497)  time: 0.3779  data: 0.0004  max mem: 31303
Epoch: [9]  [5003/5004]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000026  loss: 1.9473 (1.9185)  class_acc: 0.7812 (0.7653)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5575 (4.9474)  time: 0.3765  data: 0.0004  max mem: 31303
Epoch: [9] Total time: 0:32:31 (0.3900 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000026  loss: 1.9473 (1.9208)  class_acc: 0.7812 (0.7648)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5575 (4.9474)
Test:  [ 0/25]  eta: 0:04:54  loss: 0.3883 (0.3883)  acc1: 90.8000 (90.8000)  acc5: 99.2000 (99.2000)  time: 11.7616  data: 11.0343  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5518 (0.5363)  acc1: 86.8000 (87.7091)  acc5: 97.6000 (98.0000)  time: 1.7017  data: 1.0034  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6308 (0.6155)  acc1: 85.2000 (85.3333)  acc5: 97.2000 (97.4857)  time: 0.6948  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6796 (0.6291)  acc1: 85.2000 (84.7840)  acc5: 96.8000 (97.3920)  time: 0.6940  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1451 s / it)
* Acc@1 85.194 Acc@5 97.314 loss 0.619
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.24%
Test:  [ 0/25]  eta: 0:04:34  loss: 0.4158 (0.4158)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 10.9958  data: 10.2704  max mem: 31303
Test:  [10/25]  eta: 0:00:24  loss: 0.5669 (0.5589)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1818)  time: 1.6303  data: 0.9339  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6513 (0.6366)  acc1: 84.8000 (85.4476)  acc5: 97.2000 (97.5810)  time: 0.6936  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6713 (0.6481)  acc1: 84.8000 (84.9280)  acc5: 96.8000 (97.5200)  time: 0.6935  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.1141 s / it)
* Acc@1 85.512 Acc@5 97.516 loss 0.637
Accuracy of the model EMA on 50000 test images: 85.5%
Epoch: [10]  [   0/5004]  eta: 3:39:09  lr: 0.000026  min_lr: 0.000026  loss: 1.6526 (1.6526)  class_acc: 0.8750 (0.8750)  weight_decay: 0.0500 (0.0500)  time: 2.6278  data: 1.5958  max mem: 31303
Epoch: [10]  [ 200/5004]  eta: 0:31:51  lr: 0.000025  min_lr: 0.000025  loss: 1.9091 (1.9159)  class_acc: 0.7500 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9911 (4.9488)  time: 0.3940  data: 0.0002  max mem: 31303
Epoch: [10]  [ 400/5004]  eta: 0:30:06  lr: 0.000025  min_lr: 0.000025  loss: 1.9313 (1.9157)  class_acc: 0.7500 (0.7673)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1734 (4.7244)  time: 0.3829  data: 0.0002  max mem: 31303
Epoch: [10]  [ 600/5004]  eta: 0:28:41  lr: 0.000025  min_lr: 0.000025  loss: 1.7632 (1.9132)  class_acc: 0.8125 (0.7683)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2452 (4.6665)  time: 0.3828  data: 0.0002  max mem: 31303
Epoch: [10]  [ 800/5004]  eta: 0:27:18  lr: 0.000024  min_lr: 0.000024  loss: 1.8742 (1.9092)  class_acc: 0.7500 (0.7694)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0583 (4.6455)  time: 0.3827  data: 0.0002  max mem: 31303
Epoch: [10]  [1000/5004]  eta: 0:25:59  lr: 0.000024  min_lr: 0.000024  loss: 1.8156 (1.9144)  class_acc: 0.7812 (0.7678)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6850 (4.7403)  time: 0.3850  data: 0.0002  max mem: 31303
Epoch: [10]  [1200/5004]  eta: 0:24:40  lr: 0.000024  min_lr: 0.000024  loss: 1.8330 (1.9118)  class_acc: 0.7812 (0.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6261 (4.8181)  time: 0.3900  data: 0.0002  max mem: 31303
Epoch: [10]  [1400/5004]  eta: 0:23:23  lr: 0.000023  min_lr: 0.000023  loss: 1.8851 (1.9102)  class_acc: 0.7812 (0.7693)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4275 (4.9502)  time: 0.3832  data: 0.0002  max mem: 31303
Epoch: [10]  [1600/5004]  eta: 0:22:03  lr: 0.000023  min_lr: 0.000023  loss: 1.9824 (1.9117)  class_acc: 0.7500 (0.7688)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1705 (4.9472)  time: 0.3837  data: 0.0002  max mem: 31303
Epoch: [10]  [1800/5004]  eta: 0:20:45  lr: 0.000023  min_lr: 0.000023  loss: 1.8835 (1.9119)  class_acc: 0.7812 (0.7688)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4446 (4.9799)  time: 0.3837  data: 0.0002  max mem: 31303
Epoch: [10]  [2000/5004]  eta: 0:19:27  lr: 0.000022  min_lr: 0.000022  loss: 1.8584 (1.9107)  class_acc: 0.7500 (0.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9532 (4.9323)  time: 0.3866  data: 0.0002  max mem: 31303
Epoch: [10]  [2200/5004]  eta: 0:18:09  lr: 0.000022  min_lr: 0.000022  loss: 1.8311 (1.9087)  class_acc: 0.7812 (0.7695)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1221 (4.9378)  time: 0.3839  data: 0.0001  max mem: 31303
Epoch: [10]  [2400/5004]  eta: 0:16:51  lr: 0.000022  min_lr: 0.000022  loss: 1.8678 (1.9088)  class_acc: 0.7500 (0.7695)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6735 (4.9464)  time: 0.3853  data: 0.0002  max mem: 31303
Epoch: [10]  [2600/5004]  eta: 0:15:34  lr: 0.000022  min_lr: 0.000022  loss: 2.0388 (1.9124)  class_acc: 0.7500 (0.7682)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5641 (4.9642)  time: 0.3827  data: 0.0002  max mem: 31303
Epoch: [10]  [2800/5004]  eta: 0:14:15  lr: 0.000021  min_lr: 0.000021  loss: 1.9395 (1.9150)  class_acc: 0.7500 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4085 (4.9500)  time: 0.3822  data: 0.0002  max mem: 31303
Epoch: [10]  [3000/5004]  eta: 0:12:59  lr: 0.000021  min_lr: 0.000021  loss: 1.8680 (1.9126)  class_acc: 0.7812 (0.7681)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4498 (4.9464)  time: 0.3933  data: 0.0002  max mem: 31303
Epoch: [10]  [3200/5004]  eta: 0:11:41  lr: 0.000021  min_lr: 0.000021  loss: 1.7876 (1.9108)  class_acc: 0.7812 (0.7685)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1687 (4.9267)  time: 0.3852  data: 0.0002  max mem: 31303
Epoch: [10]  [3400/5004]  eta: 0:10:23  lr: 0.000020  min_lr: 0.000020  loss: 1.7967 (1.9103)  class_acc: 0.7812 (0.7683)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1884 (4.9280)  time: 0.3875  data: 0.0002  max mem: 31303
Epoch: [10]  [3600/5004]  eta: 0:09:06  lr: 0.000020  min_lr: 0.000020  loss: 1.8603 (1.9112)  class_acc: 0.7812 (0.7681)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1977 (4.9282)  time: 0.3891  data: 0.0002  max mem: 31303
Epoch: [10]  [3800/5004]  eta: 0:07:48  lr: 0.000020  min_lr: 0.000020  loss: 1.8184 (1.9121)  class_acc: 0.7812 (0.7678)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6254 (4.9143)  time: 0.3876  data: 0.0002  max mem: 31303
Epoch: [10]  [4000/5004]  eta: 0:06:30  lr: 0.000019  min_lr: 0.000019  loss: 1.8845 (1.9123)  class_acc: 0.7500 (0.7675)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3418 (4.8909)  time: 0.3847  data: 0.0002  max mem: 31303
Epoch: [10]  [4200/5004]  eta: 0:05:12  lr: 0.000019  min_lr: 0.000019  loss: 1.9446 (1.9129)  class_acc: 0.7812 (0.7673)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2953 (4.8922)  time: 0.3859  data: 0.0002  max mem: 31303
Epoch: [10]  [4400/5004]  eta: 0:03:55  lr: 0.000019  min_lr: 0.000019  loss: 1.7915 (1.9120)  class_acc: 0.8125 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7903 (4.8478)  time: 0.3825  data: 0.0002  max mem: 31303
Epoch: [10]  [4600/5004]  eta: 0:02:37  lr: 0.000019  min_lr: 0.000019  loss: 1.9806 (1.9117)  class_acc: 0.7500 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6707 (4.8933)  time: 0.3956  data: 0.0002  max mem: 31303
Epoch: [10]  [4800/5004]  eta: 0:01:19  lr: 0.000018  min_lr: 0.000018  loss: 1.8014 (1.9118)  class_acc: 0.8125 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3754 (4.8766)  time: 0.3918  data: 0.0002  max mem: 31303
Epoch: [10]  [5000/5004]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 1.9782 (1.9122)  class_acc: 0.7500 (0.7675)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2551 (4.8579)  time: 0.3879  data: 0.0007  max mem: 31303
Epoch: [10]  [5003/5004]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 1.9782 (1.9122)  class_acc: 0.7500 (0.7675)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0491 (4.8557)  time: 0.3870  data: 0.0007  max mem: 31303
Epoch: [10] Total time: 0:32:27 (0.3891 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 1.9782 (1.9105)  class_acc: 0.7500 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0491 (4.8557)
Test:  [ 0/25]  eta: 0:04:57  loss: 0.3918 (0.3918)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 11.8938  data: 11.1691  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5438 (0.5355)  acc1: 87.2000 (87.8545)  acc5: 97.6000 (97.9636)  time: 1.7148  data: 1.0156  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6270 (0.6162)  acc1: 84.8000 (85.2381)  acc5: 96.8000 (97.3524)  time: 0.6955  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6708 (0.6294)  acc1: 84.4000 (84.6880)  acc5: 96.8000 (97.2800)  time: 0.6941  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1496 s / it)
* Acc@1 85.220 Acc@5 97.288 loss 0.620
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.24%
Test:  [ 0/25]  eta: 0:04:57  loss: 0.4082 (0.4082)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.9166  data: 11.1945  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5590 (0.5505)  acc1: 86.8000 (87.8182)  acc5: 98.0000 (98.1818)  time: 1.7144  data: 1.0180  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6426 (0.6286)  acc1: 84.8000 (85.3714)  acc5: 97.2000 (97.6000)  time: 0.6940  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6707 (0.6405)  acc1: 84.4000 (84.8800)  acc5: 97.2000 (97.5360)  time: 0.6939  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1495 s / it)
* Acc@1 85.486 Acc@5 97.510 loss 0.630
Accuracy of the model EMA on 50000 test images: 85.5%
Epoch: [11]  [   0/5004]  eta: 3:07:33  lr: 0.000018  min_lr: 0.000018  loss: 1.7818 (1.7818)  class_acc: 0.8438 (0.8438)  weight_decay: 0.0500 (0.0500)  time: 2.2489  data: 1.6512  max mem: 31303
Epoch: [11]  [ 200/5004]  eta: 0:32:10  lr: 0.000018  min_lr: 0.000018  loss: 1.8521 (1.9010)  class_acc: 0.7812 (0.7696)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2504 (4.5223)  time: 0.3960  data: 0.0002  max mem: 31303
Epoch: [11]  [ 400/5004]  eta: 0:30:30  lr: 0.000017  min_lr: 0.000017  loss: 1.8327 (1.8898)  class_acc: 0.7500 (0.7719)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9566 (4.8658)  time: 0.3980  data: 0.0002  max mem: 31303
Epoch: [11]  [ 600/5004]  eta: 0:28:56  lr: 0.000017  min_lr: 0.000017  loss: 1.8844 (1.8988)  class_acc: 0.7812 (0.7691)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6713 (4.7266)  time: 0.3840  data: 0.0002  max mem: 31303
Epoch: [11]  [ 800/5004]  eta: 0:27:27  lr: 0.000017  min_lr: 0.000017  loss: 2.0190 (1.8996)  class_acc: 0.7188 (0.7687)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4630 (4.7402)  time: 0.3808  data: 0.0002  max mem: 31303
Epoch: [11]  [1000/5004]  eta: 0:26:03  lr: 0.000016  min_lr: 0.000016  loss: 1.7846 (1.9014)  class_acc: 0.7812 (0.7684)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6286 (4.7079)  time: 0.3851  data: 0.0002  max mem: 31303
Epoch: [11]  [1200/5004]  eta: 0:24:45  lr: 0.000016  min_lr: 0.000016  loss: 1.8194 (1.9008)  class_acc: 0.7812 (0.7682)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8414 (4.6609)  time: 0.3896  data: 0.0002  max mem: 31303
Epoch: [11]  [1400/5004]  eta: 0:23:27  lr: 0.000016  min_lr: 0.000016  loss: 1.7670 (1.8988)  class_acc: 0.7812 (0.7688)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3086 (4.6271)  time: 0.3854  data: 0.0002  max mem: 31303
Epoch: [11]  [1600/5004]  eta: 0:22:07  lr: 0.000016  min_lr: 0.000016  loss: 1.8478 (1.9003)  class_acc: 0.7500 (0.7683)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7823 (4.6313)  time: 0.3824  data: 0.0002  max mem: 31303
Epoch: [11]  [1800/5004]  eta: 0:20:47  lr: 0.000015  min_lr: 0.000015  loss: 1.9607 (1.9026)  class_acc: 0.7500 (0.7671)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7862 (4.6148)  time: 0.3853  data: 0.0002  max mem: 31303
Epoch: [11]  [2000/5004]  eta: 0:19:29  lr: 0.000015  min_lr: 0.000015  loss: 1.8536 (1.9055)  class_acc: 0.7500 (0.7666)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3153 (4.5866)  time: 0.3872  data: 0.0002  max mem: 31303
Epoch: [11]  [2200/5004]  eta: 0:18:11  lr: 0.000015  min_lr: 0.000015  loss: 1.8452 (1.9049)  class_acc: 0.7812 (0.7673)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9798 (4.5790)  time: 0.3870  data: 0.0002  max mem: 31303
Epoch: [11]  [2400/5004]  eta: 0:16:54  lr: 0.000015  min_lr: 0.000015  loss: 1.8453 (1.9046)  class_acc: 0.7500 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5792 (4.6227)  time: 0.3886  data: 0.0002  max mem: 31303
Epoch: [11]  [2600/5004]  eta: 0:15:35  lr: 0.000014  min_lr: 0.000014  loss: 2.0013 (1.9062)  class_acc: 0.7500 (0.7671)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5286 (4.6428)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [11]  [2800/5004]  eta: 0:14:17  lr: 0.000014  min_lr: 0.000014  loss: 1.8686 (1.9085)  class_acc: 0.7812 (0.7670)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2023 (4.6916)  time: 0.3846  data: 0.0002  max mem: 31303
Epoch: [11]  [3000/5004]  eta: 0:12:59  lr: 0.000014  min_lr: 0.000014  loss: 1.8234 (1.9065)  class_acc: 0.7812 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7790 (4.6616)  time: 0.3819  data: 0.0002  max mem: 31303
Epoch: [11]  [3200/5004]  eta: 0:11:41  lr: 0.000013  min_lr: 0.000013  loss: 2.0071 (1.9063)  class_acc: 0.7500 (0.7678)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1649 (4.6460)  time: 0.3852  data: 0.0002  max mem: 31303
Epoch: [11]  [3400/5004]  eta: 0:10:23  lr: 0.000013  min_lr: 0.000013  loss: 1.9270 (1.9067)  class_acc: 0.7500 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1136 (4.6645)  time: 0.3922  data: 0.0002  max mem: 31303
Epoch: [11]  [3600/5004]  eta: 0:09:05  lr: 0.000013  min_lr: 0.000013  loss: 1.8697 (1.9067)  class_acc: 0.7812 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6394 (4.6734)  time: 0.3855  data: 0.0002  max mem: 31303
Epoch: [11]  [3800/5004]  eta: 0:07:48  lr: 0.000013  min_lr: 0.000013  loss: 1.9268 (1.9083)  class_acc: 0.7812 (0.7673)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9120 (4.6881)  time: 0.3884  data: 0.0002  max mem: 31303
Epoch: [11]  [4000/5004]  eta: 0:06:30  lr: 0.000012  min_lr: 0.000012  loss: 1.8417 (1.9082)  class_acc: 0.7812 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6757 (4.6836)  time: 0.3840  data: 0.0002  max mem: 31303
Epoch: [11]  [4200/5004]  eta: 0:05:12  lr: 0.000012  min_lr: 0.000012  loss: 1.7950 (1.9078)  class_acc: 0.7812 (0.7680)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5071 (4.6702)  time: 0.3803  data: 0.0002  max mem: 31303
Epoch: [11]  [4400/5004]  eta: 0:03:54  lr: 0.000012  min_lr: 0.000012  loss: 1.8651 (1.9082)  class_acc: 0.7812 (0.7679)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0705 (4.6727)  time: 0.3961  data: 0.0002  max mem: 31303
Epoch: [11]  [4600/5004]  eta: 0:02:36  lr: 0.000012  min_lr: 0.000012  loss: 1.7526 (1.9091)  class_acc: 0.7812 (0.7677)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8458 (4.6873)  time: 0.3917  data: 0.0002  max mem: 31303
Epoch: [11]  [4800/5004]  eta: 0:01:19  lr: 0.000011  min_lr: 0.000011  loss: 1.9195 (1.9094)  class_acc: 0.7500 (0.7673)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7834 (4.6843)  time: 0.3936  data: 0.0002  max mem: 31303
Epoch: [11]  [5000/5004]  eta: 0:00:01  lr: 0.000011  min_lr: 0.000011  loss: 1.7609 (1.9092)  class_acc: 0.7812 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4055 (4.6834)  time: 0.3979  data: 0.0005  max mem: 31303
Epoch: [11]  [5003/5004]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000011  loss: 1.8185 (1.9092)  class_acc: 0.7812 (0.7676)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4055 (4.6830)  time: 0.3968  data: 0.0005  max mem: 31303
Epoch: [11] Total time: 0:32:24 (0.3886 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 1.8185 (1.9075)  class_acc: 0.7812 (0.7680)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4055 (4.6830)
Test:  [ 0/25]  eta: 0:04:55  loss: 0.3830 (0.3830)  acc1: 90.4000 (90.4000)  acc5: 99.6000 (99.6000)  time: 11.8148  data: 11.1062  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5509 (0.5321)  acc1: 87.2000 (87.9636)  acc5: 97.6000 (97.9636)  time: 1.7271  data: 1.0099  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6269 (0.6131)  acc1: 85.6000 (85.3524)  acc5: 96.8000 (97.3714)  time: 0.7190  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6668 (0.6259)  acc1: 84.4000 (84.8640)  acc5: 96.8000 (97.3440)  time: 0.7197  data: 0.0001  max mem: 31303
Test: Total time: 0:00:29 (1.1927 s / it)
* Acc@1 85.304 Acc@5 97.316 loss 0.616
Accuracy of the model on the 50000 test images: 85.3%
Max accuracy: 85.30%
Test:  [ 0/25]  eta: 0:04:57  loss: 0.4022 (0.4022)  acc1: 91.6000 (91.6000)  acc5: 99.6000 (99.6000)  time: 11.8900  data: 11.1670  max mem: 31303
Test:  [10/25]  eta: 0:00:25  loss: 0.5539 (0.5444)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1818)  time: 1.7118  data: 1.0156  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6368 (0.6229)  acc1: 84.8000 (85.3714)  acc5: 97.2000 (97.5810)  time: 0.6949  data: 0.0003  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6714 (0.6349)  acc1: 84.8000 (84.9120)  acc5: 97.2000 (97.5200)  time: 0.6945  data: 0.0002  max mem: 31303
Test: Total time: 0:00:28 (1.1493 s / it)
* Acc@1 85.482 Acc@5 97.496 loss 0.624
Accuracy of the model EMA on 50000 test images: 85.5%
Epoch: [12]  [   0/5004]  eta: 3:31:23  lr: 0.000011  min_lr: 0.000011  loss: 2.5690 (2.5690)  class_acc: 0.5938 (0.5938)  weight_decay: 0.0500 (0.0500)  time: 2.5347  data: 1.3233  max mem: 31303
Epoch: [12]  [ 200/5004]  eta: 0:31:53  lr: 0.000011  min_lr: 0.000011  loss: 1.8605 (1.8806)  class_acc: 0.7812 (0.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4287 (4.9662)  time: 0.3810  data: 0.0002  max mem: 31303
Epoch: [12]  [ 400/5004]  eta: 0:30:09  lr: 0.000011  min_lr: 0.000011  loss: 1.7420 (1.9047)  class_acc: 0.8125 (0.7712)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7675 (4.6239)  time: 0.3896  data: 0.0002  max mem: 31303
Epoch: [12]  [ 600/5004]  eta: 0:28:48  lr: 0.000010  min_lr: 0.000010  loss: 1.8875 (1.9087)  class_acc: 0.7500 (0.7695)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7151 (4.7256)  time: 0.3826  data: 0.0002  max mem: 31303
Epoch: [12]  [ 800/5004]  eta: 0:27:26  lr: 0.000010  min_lr: 0.000010  loss: 1.8242 (1.9065)  class_acc: 0.7812 (0.7702)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2397 (4.7441)  time: 0.3805  data: 0.0002  max mem: 31303
Epoch: [12]  [1000/5004]  eta: 0:26:01  lr: 0.000010  min_lr: 0.000010  loss: 1.7848 (1.9027)  class_acc: 0.7812 (0.7704)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9772 (4.7800)  time: 0.3811  data: 0.0002  max mem: 31303
Epoch: [12]  [1200/5004]  eta: 0:24:41  lr: 0.000010  min_lr: 0.000010  loss: 1.8808 (1.8956)  class_acc: 0.7500 (0.7715)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5604 (4.7313)  time: 0.3825  data: 0.0002  max mem: 31303
Epoch: [12]  [1400/5004]  eta: 0:23:21  lr: 0.000009  min_lr: 0.000009  loss: 1.7744 (1.8939)  class_acc: 0.7812 (0.7711)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3289 (4.7970)  time: 0.3840  data: 0.0002  max mem: 31303
Epoch: [12]  [1600/5004]  eta: 0:22:02  lr: 0.000009  min_lr: 0.000009  loss: 1.8756 (1.8919)  class_acc: 0.7500 (0.7715)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6330 (4.7351)  time: 0.3936  data: 0.0002  max mem: 31303
Epoch: [12]  [1800/5004]  eta: 0:20:44  lr: 0.000009  min_lr: 0.000009  loss: 1.7872 (1.8950)  class_acc: 0.7812 (0.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3989 (4.7109)  time: 0.3826  data: 0.0002  max mem: 31303
Epoch: [12]  [2000/5004]  eta: 0:19:26  lr: 0.000009  min_lr: 0.000009  loss: 1.8058 (1.8973)  class_acc: 0.8125 (0.7704)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1020 (4.7262)  time: 0.3825  data: 0.0002  max mem: 31303
Epoch: [12]  [2200/5004]  eta: 0:18:08  lr: 0.000009  min_lr: 0.000009  loss: 1.9297 (1.8997)  class_acc: 0.7188 (0.7701)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9016 (4.7380)  time: 0.3833  data: 0.0002  max mem: 31303
Epoch: [12]  [2400/5004]  eta: 0:16:52  lr: 0.000008  min_lr: 0.000008  loss: 1.7356 (1.8974)  class_acc: 0.7812 (0.7706)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8957 (4.8411)  time: 0.4080  data: 0.0002  max mem: 31303
Epoch: [12]  [2600/5004]  eta: 0:15:34  lr: 0.000008  min_lr: 0.000008  loss: 1.8961 (1.8961)  class_acc: 0.7500 (0.7712)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7666 (4.8299)  time: 0.3906  data: 0.0002  max mem: 31303
Epoch: [12]  [2800/5004]  eta: 0:14:16  lr: 0.000008  min_lr: 0.000008  loss: 1.9178 (1.8971)  class_acc: 0.7500 (0.7703)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6055 (4.8383)  time: 0.3894  data: 0.0002  max mem: 31303
Epoch: [12]  [3000/5004]  eta: 0:12:58  lr: 0.000008  min_lr: 0.000008  loss: 1.8745 (1.8980)  class_acc: 0.7812 (0.7701)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6942 (4.8508)  time: 0.3876  data: 0.0002  max mem: 31303
Epoch: [12]  [3200/5004]  eta: 0:11:41  lr: 0.000007  min_lr: 0.000007  loss: 1.8587 (1.8987)  class_acc: 0.7812 (0.7697)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4782 (4.8174)  time: 0.4010  data: 0.0002  max mem: 31303
Epoch: [12]  [3400/5004]  eta: 0:10:23  lr: 0.000007  min_lr: 0.000007  loss: 1.9404 (1.9004)  class_acc: 0.7500 (0.7693)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4526 (4.7831)  time: 0.3804  data: 0.0002  max mem: 31303
Epoch: [12]  [3600/5004]  eta: 0:09:05  lr: 0.000007  min_lr: 0.000007  loss: 1.8383 (1.9018)  class_acc: 0.7812 (0.7691)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5849 (4.7727)  time: 0.3856  data: 0.0002  max mem: 31303
Epoch: [12]  [3800/5004]  eta: 0:07:47  lr: 0.000007  min_lr: 0.000007  loss: 1.8648 (1.9010)  class_acc: 0.7812 (0.7694)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8684 (4.7535)  time: 0.3922  data: 0.0002  max mem: 31303
Epoch: [12]  [4000/5004]  eta: 0:06:29  lr: 0.000007  min_lr: 0.000007  loss: 1.8550 (1.9009)  class_acc: 0.7812 (0.7696)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7808 (4.7376)  time: 0.4191  data: 0.0002  max mem: 31303
Epoch: [12]  [4200/5004]  eta: 0:05:12  lr: 0.000006  min_lr: 0.000006  loss: 1.8117 (1.9010)  class_acc: 0.7812 (0.7694)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0517 (4.7227)  time: 0.3881  data: 0.0002  max mem: 31303
Epoch: [12]  [4400/5004]  eta: 0:03:54  lr: 0.000006  min_lr: 0.000006  loss: 1.8891 (1.9014)  class_acc: 0.7812 (0.7694)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8117 (4.7548)  time: 0.3872  data: 0.0002  max mem: 31303
Epoch: [12]  [4600/5004]  eta: 0:02:37  lr: 0.000006  min_lr: 0.000006  loss: 1.7813 (1.9003)  class_acc: 0.7812 (0.7698)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6365 (inf)  time: 0.3934  data: 0.0002  max mem: 31303
Epoch: [12]  [4800/5004]  eta: 0:01:19  lr: 0.000006  min_lr: 0.000006  loss: 1.8271 (1.8997)  class_acc: 0.7812 (0.7701)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4764 (inf)  time: 0.4213  data: 0.0002  max mem: 31303
Epoch: [12]  [5000/5004]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 1.8822 (1.8996)  class_acc: 0.7500 (0.7699)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0007 (inf)  time: 0.3764  data: 0.0005  max mem: 31303
Epoch: [12]  [5003/5004]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 1.8825 (1.8997)  class_acc: 0.7188 (0.7699)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6126 (inf)  time: 0.3752  data: 0.0005  max mem: 31303
Epoch: [12] Total time: 0:32:28 (0.3894 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000006  loss: 1.8825 (1.9038)  class_acc: 0.7188 (0.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6126 (inf)
Test:  [ 0/25]  eta: 0:03:42  loss: 0.3852 (0.3852)  acc1: 90.4000 (90.4000)  acc5: 99.6000 (99.6000)  time: 8.9024  data: 8.1754  max mem: 31303
Test:  [10/25]  eta: 0:00:24  loss: 0.5490 (0.5325)  acc1: 86.8000 (87.8909)  acc5: 97.6000 (98.0364)  time: 1.6578  data: 0.9403  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6314 (0.6157)  acc1: 85.2000 (85.4095)  acc5: 97.2000 (97.4095)  time: 0.8302  data: 0.1084  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6747 (0.6290)  acc1: 85.2000 (84.8480)  acc5: 96.8000 (97.3600)  time: 0.7196  data: 0.0001  max mem: 31303
Test: Total time: 0:00:28 (1.1415 s / it)
* Acc@1 85.268 Acc@5 97.338 loss 0.619
Accuracy of the model on the 50000 test images: 85.3%
Max accuracy: 85.30%
Test:  [ 0/25]  eta: 0:04:10  loss: 0.3977 (0.3977)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 10.0230  data: 9.3128  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5497 (0.5398)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1818)  time: 1.5429  data: 0.8469  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6327 (0.6186)  acc1: 84.8000 (85.3143)  acc5: 97.2000 (97.5429)  time: 0.6945  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6729 (0.6308)  acc1: 84.4000 (84.8320)  acc5: 97.2000 (97.4880)  time: 0.6943  data: 0.0001  max mem: 31303
Test: Total time: 0:00:26 (1.0736 s / it)
* Acc@1 85.460 Acc@5 97.480 loss 0.621
Accuracy of the model EMA on 50000 test images: 85.5%
Epoch: [13]  [   0/5004]  eta: 3:19:10  lr: 0.000006  min_lr: 0.000006  loss: 1.6838 (1.6838)  class_acc: 0.8750 (0.8750)  weight_decay: 0.0500 (0.0500)  time: 2.3882  data: 1.7479  max mem: 31303
Epoch: [13]  [ 200/5004]  eta: 0:31:44  lr: 0.000005  min_lr: 0.000005  loss: 1.8364 (1.9034)  class_acc: 0.8125 (0.7732)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5428 (4.0702)  time: 0.3816  data: 0.0002  max mem: 31303
Epoch: [13]  [ 400/5004]  eta: 0:30:07  lr: 0.000005  min_lr: 0.000005  loss: 1.8488 (1.8897)  class_acc: 0.7812 (0.7750)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2827 (4.3768)  time: 0.3829  data: 0.0002  max mem: 31303
Epoch: [13]  [ 600/5004]  eta: 0:28:44  lr: 0.000005  min_lr: 0.000005  loss: 1.7724 (1.8868)  class_acc: 0.7812 (0.7756)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3272 (4.3357)  time: 0.4097  data: 0.0002  max mem: 31303
Epoch: [13]  [ 800/5004]  eta: 0:27:18  lr: 0.000005  min_lr: 0.000005  loss: 1.9183 (1.8898)  class_acc: 0.7500 (0.7734)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9600 (4.3138)  time: 0.3904  data: 0.0002  max mem: 31303
Epoch: [13]  [1000/5004]  eta: 0:25:58  lr: 0.000005  min_lr: 0.000005  loss: 1.8211 (1.8895)  class_acc: 0.7500 (0.7737)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3144 (4.4198)  time: 0.3936  data: 0.0002  max mem: 31303
Epoch: [13]  [1200/5004]  eta: 0:24:40  lr: 0.000005  min_lr: 0.000005  loss: 1.8281 (1.8899)  class_acc: 0.7812 (0.7719)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4828 (4.4471)  time: 0.3975  data: 0.0002  max mem: 31303
Epoch: [13]  [1400/5004]  eta: 0:23:24  lr: 0.000004  min_lr: 0.000004  loss: 1.9119 (1.8898)  class_acc: 0.7500 (0.7721)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6185 (4.4969)  time: 0.4179  data: 0.0003  max mem: 31303
Epoch: [13]  [1600/5004]  eta: 0:22:06  lr: 0.000004  min_lr: 0.000004  loss: 1.8686 (1.8907)  class_acc: 0.7500 (0.7716)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9807 (4.5190)  time: 0.3908  data: 0.0002  max mem: 31303
Epoch: [13]  [1800/5004]  eta: 0:20:48  lr: 0.000004  min_lr: 0.000004  loss: 1.8947 (1.8916)  class_acc: 0.7812 (0.7719)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4943 (4.5492)  time: 0.3815  data: 0.0003  max mem: 31303
Epoch: [13]  [2000/5004]  eta: 0:19:29  lr: 0.000004  min_lr: 0.000004  loss: 1.8937 (1.8922)  class_acc: 0.7812 (0.7717)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6099 (4.5414)  time: 0.3820  data: 0.0002  max mem: 31303
Epoch: [13]  [2200/5004]  eta: 0:18:10  lr: 0.000004  min_lr: 0.000004  loss: 1.9643 (1.8924)  class_acc: 0.7812 (0.7718)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3429 (4.5179)  time: 0.4072  data: 0.0002  max mem: 31303
Epoch: [13]  [2400/5004]  eta: 0:16:51  lr: 0.000004  min_lr: 0.000004  loss: 1.8875 (1.8915)  class_acc: 0.7500 (0.7716)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3545 (4.5146)  time: 0.3812  data: 0.0002  max mem: 31303
Epoch: [13]  [2600/5004]  eta: 0:15:33  lr: 0.000004  min_lr: 0.000004  loss: 1.8458 (1.8913)  class_acc: 0.7812 (0.7716)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2181 (4.5302)  time: 0.3833  data: 0.0002  max mem: 31303
Epoch: [13]  [2800/5004]  eta: 0:14:16  lr: 0.000003  min_lr: 0.000003  loss: 1.7933 (1.8906)  class_acc: 0.7812 (0.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1948 (4.5098)  time: 0.3813  data: 0.0002  max mem: 31303
Epoch: [13]  [3000/5004]  eta: 0:12:58  lr: 0.000003  min_lr: 0.000003  loss: 1.8941 (1.8929)  class_acc: 0.7500 (0.7713)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7298 (4.4847)  time: 0.4167  data: 0.0002  max mem: 31303
Epoch: [13]  [3200/5004]  eta: 0:11:41  lr: 0.000003  min_lr: 0.000003  loss: 1.9209 (1.8941)  class_acc: 0.7500 (0.7708)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4375 (4.4999)  time: 0.3955  data: 0.0002  max mem: 31303
Epoch: [13]  [3400/5004]  eta: 0:10:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8506 (1.8947)  class_acc: 0.7812 (0.7709)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6010 (4.4914)  time: 0.3835  data: 0.0002  max mem: 31303
Epoch: [13]  [3600/5004]  eta: 0:09:06  lr: 0.000003  min_lr: 0.000003  loss: 1.9098 (1.8943)  class_acc: 0.7500 (0.7709)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8864 (4.4804)  time: 0.3843  data: 0.0002  max mem: 31303
Epoch: [13]  [3800/5004]  eta: 0:07:48  lr: 0.000003  min_lr: 0.000003  loss: 1.8967 (1.8934)  class_acc: 0.7500 (0.7711)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1841 (4.4983)  time: 0.3986  data: 0.0002  max mem: 31303
Epoch: [13]  [4000/5004]  eta: 0:06:30  lr: 0.000003  min_lr: 0.000003  loss: 1.8545 (1.8934)  class_acc: 0.7812 (0.7710)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8260 (4.5048)  time: 0.3866  data: 0.0002  max mem: 31303
Epoch: [13]  [4200/5004]  eta: 0:05:12  lr: 0.000003  min_lr: 0.000003  loss: 1.9920 (1.8939)  class_acc: 0.7188 (0.7707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3885 (4.5125)  time: 0.3812  data: 0.0002  max mem: 31303
Epoch: [13]  [4400/5004]  eta: 0:03:54  lr: 0.000003  min_lr: 0.000003  loss: 1.8502 (1.8939)  class_acc: 0.7500 (0.7707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1632 (4.5183)  time: 0.3801  data: 0.0002  max mem: 31303
Epoch: [13]  [4600/5004]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000002  loss: 1.9222 (1.8942)  class_acc: 0.7500 (0.7706)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9661 (4.5009)  time: 0.3808  data: 0.0002  max mem: 31303
Epoch: [13]  [4800/5004]  eta: 0:01:19  lr: 0.000002  min_lr: 0.000002  loss: 1.8928 (1.8946)  class_acc: 0.7812 (0.7708)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0673 (4.4926)  time: 0.3802  data: 0.0002  max mem: 31303
Epoch: [13]  [5000/5004]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000002  loss: 1.9114 (1.8951)  class_acc: 0.7812 (0.7707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2597 (4.5024)  time: 0.3777  data: 0.0006  max mem: 31303
Epoch: [13]  [5003/5004]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000002  loss: 1.9114 (1.8950)  class_acc: 0.7500 (0.7707)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6274 (4.5014)  time: 0.3766  data: 0.0006  max mem: 31303
Epoch: [13] Total time: 0:32:23 (0.3884 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000002  loss: 1.9114 (1.9017)  class_acc: 0.7500 (0.7697)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6274 (4.5014)
Test:  [ 0/25]  eta: 0:04:11  loss: 0.3897 (0.3897)  acc1: 90.8000 (90.8000)  acc5: 99.6000 (99.6000)  time: 10.0651  data: 9.3329  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5460 (0.5344)  acc1: 86.8000 (87.8909)  acc5: 97.6000 (97.9636)  time: 1.5869  data: 0.8894  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6331 (0.6180)  acc1: 85.2000 (85.2191)  acc5: 96.8000 (97.3333)  time: 0.7161  data: 0.0226  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6791 (0.6308)  acc1: 84.8000 (84.7200)  acc5: 96.8000 (97.2800)  time: 0.6935  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.0929 s / it)
* Acc@1 85.230 Acc@5 97.296 loss 0.621
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.30%
Test:  [ 0/25]  eta: 0:02:37  loss: 0.3945 (0.3945)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 6.3184  data: 5.5944  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5468 (0.5368)  acc1: 86.8000 (87.9273)  acc5: 97.6000 (98.1091)  time: 1.5912  data: 0.8900  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6301 (0.6155)  acc1: 84.8000 (85.3333)  acc5: 97.2000 (97.5429)  time: 0.9063  data: 0.2098  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6719 (0.6279)  acc1: 84.4000 (84.8480)  acc5: 97.2000 (97.4720)  time: 0.6942  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.0980 s / it)
* Acc@1 85.444 Acc@5 97.470 loss 0.618
Accuracy of the model EMA on 50000 test images: 85.4%
Epoch: [14]  [   0/5004]  eta: 3:25:02  lr: 0.000002  min_lr: 0.000002  loss: 1.9703 (1.9703)  class_acc: 0.7188 (0.7188)  weight_decay: 0.0500 (0.0500)  time: 2.4585  data: 1.9485  max mem: 31303
Epoch: [14]  [ 200/5004]  eta: 0:32:19  lr: 0.000002  min_lr: 0.000002  loss: 1.8157 (1.9220)  class_acc: 0.7812 (0.7600)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9925 (4.3750)  time: 0.3842  data: 0.0002  max mem: 31303
Epoch: [14]  [ 400/5004]  eta: 0:30:22  lr: 0.000002  min_lr: 0.000002  loss: 1.7895 (1.9162)  class_acc: 0.7812 (0.7664)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4912 (4.5255)  time: 0.4136  data: 0.0002  max mem: 31303
Epoch: [14]  [ 600/5004]  eta: 0:28:48  lr: 0.000002  min_lr: 0.000002  loss: 1.8844 (1.9110)  class_acc: 0.7500 (0.7674)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2408 (4.6517)  time: 0.3867  data: 0.0002  max mem: 31303
Epoch: [14]  [ 800/5004]  eta: 0:27:32  lr: 0.000002  min_lr: 0.000002  loss: 1.8736 (1.9048)  class_acc: 0.7812 (0.7694)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7164 (4.5595)  time: 0.3917  data: 0.0002  max mem: 31303
Epoch: [14]  [1000/5004]  eta: 0:26:15  lr: 0.000002  min_lr: 0.000002  loss: 1.8525 (1.9032)  class_acc: 0.7500 (0.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0760 (4.4775)  time: 0.3862  data: 0.0002  max mem: 31303
Epoch: [14]  [1200/5004]  eta: 0:24:54  lr: 0.000002  min_lr: 0.000002  loss: 1.7720 (1.9002)  class_acc: 0.7812 (0.7696)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1054 (4.5096)  time: 0.4099  data: 0.0002  max mem: 31303
Epoch: [14]  [1400/5004]  eta: 0:23:34  lr: 0.000002  min_lr: 0.000002  loss: 1.8315 (1.9003)  class_acc: 0.7812 (0.7693)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2017 (4.5293)  time: 0.3940  data: 0.0002  max mem: 31303
Epoch: [14]  [1600/5004]  eta: 0:22:15  lr: 0.000002  min_lr: 0.000002  loss: 1.9130 (1.9000)  class_acc: 0.7500 (0.7699)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7090 (4.5248)  time: 0.3900  data: 0.0002  max mem: 31303
Epoch: [14]  [1800/5004]  eta: 0:20:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8162 (1.8978)  class_acc: 0.7812 (0.7700)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3280 (4.5374)  time: 0.3831  data: 0.0002  max mem: 31303
Epoch: [14]  [2000/5004]  eta: 0:19:35  lr: 0.000001  min_lr: 0.000001  loss: 1.8449 (1.8979)  class_acc: 0.7812 (0.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9018 (4.5172)  time: 0.3934  data: 0.0002  max mem: 31303
Epoch: [14]  [2200/5004]  eta: 0:18:16  lr: 0.000001  min_lr: 0.000001  loss: 1.9014 (1.8985)  class_acc: 0.7812 (0.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5220 (4.4894)  time: 0.3858  data: 0.0002  max mem: 31303
Epoch: [14]  [2400/5004]  eta: 0:16:58  lr: 0.000001  min_lr: 0.000001  loss: 1.7650 (1.8983)  class_acc: 0.7500 (0.7702)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4935 (4.4659)  time: 0.3843  data: 0.0002  max mem: 31303
Epoch: [14]  [2600/5004]  eta: 0:15:40  lr: 0.000001  min_lr: 0.000001  loss: 1.8206 (1.8974)  class_acc: 0.7812 (0.7704)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4727 (4.4506)  time: 0.3833  data: 0.0002  max mem: 31303
Epoch: [14]  [2800/5004]  eta: 0:14:20  lr: 0.000001  min_lr: 0.000001  loss: 1.8253 (1.8955)  class_acc: 0.7500 (0.7711)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6654 (4.4389)  time: 0.3840  data: 0.0002  max mem: 31303
Epoch: [14]  [3000/5004]  eta: 0:13:02  lr: 0.000001  min_lr: 0.000001  loss: 1.8731 (1.8972)  class_acc: 0.7500 (0.7707)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6827 (4.4565)  time: 0.3877  data: 0.0002  max mem: 31303
Epoch: [14]  [3200/5004]  eta: 0:11:44  lr: 0.000001  min_lr: 0.000001  loss: 1.9234 (1.8978)  class_acc: 0.7812 (0.7706)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3338 (4.4350)  time: 0.3822  data: 0.0002  max mem: 31303
Epoch: [14]  [3400/5004]  eta: 0:10:26  lr: 0.000001  min_lr: 0.000001  loss: 1.8931 (1.8979)  class_acc: 0.7500 (0.7703)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9621 (4.4458)  time: 0.3845  data: 0.0002  max mem: 31303
Epoch: [14]  [3600/5004]  eta: 0:09:07  lr: 0.000001  min_lr: 0.000001  loss: 1.9022 (1.8977)  class_acc: 0.7812 (0.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2825 (4.4885)  time: 0.3843  data: 0.0002  max mem: 31303
Epoch: [14]  [3800/5004]  eta: 0:07:49  lr: 0.000001  min_lr: 0.000001  loss: 1.8614 (1.8970)  class_acc: 0.7812 (0.7708)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6516 (4.4641)  time: 0.3943  data: 0.0002  max mem: 31303
Epoch: [14]  [4000/5004]  eta: 0:06:31  lr: 0.000001  min_lr: 0.000001  loss: 1.9036 (1.8970)  class_acc: 0.7812 (0.7709)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5047 (4.4573)  time: 0.3813  data: 0.0002  max mem: 31303
Epoch: [14]  [4200/5004]  eta: 0:05:13  lr: 0.000001  min_lr: 0.000001  loss: 1.8766 (1.8971)  class_acc: 0.7500 (0.7708)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9114 (4.4619)  time: 0.3848  data: 0.0002  max mem: 31303
Epoch: [14]  [4400/5004]  eta: 0:03:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8275 (1.8982)  class_acc: 0.8125 (0.7703)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0376 (4.4691)  time: 0.3866  data: 0.0002  max mem: 31303
Epoch: [14]  [4600/5004]  eta: 0:02:37  lr: 0.000001  min_lr: 0.000001  loss: 2.0344 (1.8988)  class_acc: 0.7500 (0.7700)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6220 (4.4821)  time: 0.3821  data: 0.0002  max mem: 31303
Epoch: [14]  [4800/5004]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000001  loss: 1.9091 (1.8987)  class_acc: 0.7500 (0.7702)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8843 (4.4686)  time: 0.3848  data: 0.0002  max mem: 31303
Epoch: [14]  [5000/5004]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.8236 (1.8990)  class_acc: 0.7812 (0.7703)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6707 (4.4500)  time: 0.3772  data: 0.0006  max mem: 31303
Epoch: [14]  [5003/5004]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.8590 (1.8991)  class_acc: 0.7812 (0.7702)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6707 (4.4486)  time: 0.3765  data: 0.0005  max mem: 31303
Epoch: [14] Total time: 0:32:32 (0.3901 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.8590 (1.9000)  class_acc: 0.7812 (0.7697)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6707 (4.4486)
Test:  [ 0/25]  eta: 0:05:09  loss: 0.3875 (0.3875)  acc1: 90.8000 (90.8000)  acc5: 99.6000 (99.6000)  time: 12.3618  data: 11.6534  max mem: 31303
Test:  [10/25]  eta: 0:00:26  loss: 0.5472 (0.5306)  acc1: 86.8000 (87.8545)  acc5: 97.6000 (97.9273)  time: 1.7693  data: 1.0596  max mem: 31303
Test:  [20/25]  eta: 0:00:06  loss: 0.6269 (0.6115)  acc1: 85.2000 (85.2952)  acc5: 96.8000 (97.3333)  time: 0.7025  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6690 (0.6255)  acc1: 84.8000 (84.7520)  acc5: 96.8000 (97.2960)  time: 0.6982  data: 0.0001  max mem: 31303
Test: Total time: 0:00:29 (1.1743 s / it)
* Acc@1 85.236 Acc@5 97.322 loss 0.615
Accuracy of the model on the 50000 test images: 85.2%
Max accuracy: 85.30%
Test:  [ 0/25]  eta: 0:03:09  loss: 0.3921 (0.3921)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 7.5730  data: 6.8467  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5447 (0.5343)  acc1: 86.8000 (87.8909)  acc5: 98.0000 (98.1091)  time: 1.5463  data: 0.8484  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6278 (0.6132)  acc1: 84.8000 (85.2952)  acc5: 97.2000 (97.5238)  time: 0.8182  data: 0.1243  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6701 (0.6258)  acc1: 84.0000 (84.8160)  acc5: 97.2000 (97.4560)  time: 0.6929  data: 0.0001  max mem: 31303
Test: Total time: 0:00:26 (1.0749 s / it)
* Acc@1 85.432 Acc@5 97.456 loss 0.616
Accuracy of the model EMA on 50000 test images: 85.4%
Test:  [ 0/25]  eta: 0:04:16  loss: 0.3757 (0.3757)  acc1: 91.2000 (91.2000)  acc5: 99.6000 (99.6000)  time: 10.2408  data: 9.5327  max mem: 31303
Test:  [10/25]  eta: 0:00:23  loss: 0.5435 (0.5219)  acc1: 86.8000 (87.8545)  acc5: 98.0000 (98.1091)  time: 1.5630  data: 0.8669  max mem: 31303
Test:  [20/25]  eta: 0:00:05  loss: 0.6155 (0.6004)  acc1: 84.8000 (85.3524)  acc5: 97.2000 (97.4667)  time: 0.6947  data: 0.0002  max mem: 31303
Test:  [24/25]  eta: 0:00:01  loss: 0.6603 (0.6138)  acc1: 84.8000 (84.8640)  acc5: 96.8000 (97.4080)  time: 0.6945  data: 0.0001  max mem: 31303
Test: Total time: 0:00:27 (1.0889 s / it)
* Acc@1 85.354 Acc@5 97.418 loss 0.604
Accuracy of the model EMA on 50000 test images: 85.4%
Training time 8:32:49
