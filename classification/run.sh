# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 300 --batch_size 128 --lr 4.0e-3 --update_freq 4 --model_ema false --model_ema_eval false --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_pico_0.8G --model pico --mixup 0.0 --cutmix 0.1 --drop_path 0.00 | tee -a log_pico_0.8G.txt
# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 300 --batch_size 128 --lr 4.0e-3 --update_freq 4 --model_ema false --model_ema_eval false --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_nano_1.4G --model nano --mixup 0.2 --cutmix 0.3 --drop_path 0.05 | tee -a log_nano_1.4G.txt
# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 300 --batch_size 128 --lr 4.0e-3 --update_freq 4 --model_ema false --model_ema_eval false --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_tiny_2.5G --model tiny --mixup 0.4 --cutmix 0.5 --drop_path 0.10 | tee -a log_tiny_2.5G.txt
# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 300 --batch_size 128 --lr 4.0e-3 --update_freq 4 --model_ema false --model_ema_eval false --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_small_4.4G --model small --mixup 0.8 --cutmix 1.0 --drop_path 0.20 | tee -a log_small_4.4G.txt
# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 300 --batch_size 128 --lr 4.0e-3 --update_freq 4 --model_ema false --model_ema_eval false --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_base_8.5G --model base --mixup 0.8 --cutmix 1.0 --drop_path 0.35 | tee -a log_base_8.5G.txt

# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 15 --batch_size 32 --lr 5.0e-5 --update_freq 4 --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_base_256_11.1G --model base --mixup 0.0 --cutmix 0.0 --drop_path 0.55 --warmup_epochs 5 --model_ema true --model_ema_decay 0.9999 --model_ema_eval true --input_size 256 --finetune ./checkpoint_base_8.5G.pth | tee -a log_base_256_11.1G.txt

# python -m torch.distributed.run --nproc_per_node=8 main.py --epochs 15 --batch_size 32 --lr 5.0e-5 --update_freq 4 --use_amp true --data_path /dev/shm/imagenet --output_dir ./checkpoint_base_384_25.1G --model base --mixup 0.0 --cutmix 0.0 --drop_path 0.70 --warmup_epochs 5 --model_ema true --model_ema_decay 0.9999 --model_ema_eval true --input_size 384 --finetune ./checkpoint_base_8.5G.pth | tee -a log_base_384_25.1G.txt
